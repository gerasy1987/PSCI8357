---
title: "Regression?<br>Regression!"
subtitle: "PSCI 8357 - Stats II"
author: Georgiy Syunyaev
institute: "Department of Political Science, Vanderbilt University"
date: today
date-format: long
format: 
  revealjs:
    toc: true
    toc-depth: 1
    toc-title: "Plan"
    slide-number: c/t
    # preview-links: true
    embed-resources: true
    link-external-icon: true
    link-external-newwindow: true
    html-math-method: mathjax
    # logo: images/wzb_logo.png
    self-contained-math: true
    css: ../_supp/styles.css
    theme: [serif,"../_supp/custom.scss"]
    incremental: false
    self-contained: true
    citations-hover: true
    fragments: true
    # progress: true
    scrollable: false
    transition: fade
    reference-location: document
    fig-cap-location: top
fontsize: 26px
editor: source
aspectratio: 169
bibliography: ../_supp/psci8357.bib
---


## {data-visibility="hidden"}

```css
<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: true}});</script>
```

\(
  \def\E{{\mathbb{E}}}
  \def\Pr{{\textrm{Pr}}}
  \def\var{{\mathbb{V}}}
  \def\cov{{\mathrm{cov}}}
  \def\corr{{\mathrm{corr}}}
  \def\argmin{{\arg\!\min}}
  \def\argmax{{\arg\!\max}}
  \def\qedknitr{{\hfill\rule{1.2ex}{1.2ex}}}
  \def\given{{\:\vert\:}}
  \def\indep{{\mbox{$\perp\!\!\!\perp$}}}
\)

```{r}
#|  label: preamble
#|  include: false

# load necessary libraries
pacman::p_load(tidyverse)

# set theme for plots
thematic::thematic_rmd(bg = "#f0f1eb", fg = "#111111", accent = "#111111")
```


## From DiM to Regression

<br>

- So far we considered difference in means as our naive estimator of causal quantities

. . .

- This week we will see, that we might use "agnostic" regression to estimate causal estimands as well

  - this makes our life easier, especially if we would like to rely on [conditional ignorability]{.highlight} assumption

. . .

- [BUT]{.note} this only solves the estimation problem

  - We still have to make assumptions to achieve causal identification!

. . .

- [Problem]{.alert}: If we want to learn about relationship between $X$ and $Y$

  - the ideal is to learn about $f_{YX}(\cdot)$, 
  - instead we have to learn about $[Y \given X]$.

# CEF

## Conditional Expectation Function (CEF)

:::{.callout-important icon="false" title="CEF"}

The [CEF]{.highlight}, $\E [Y_i \given X_i]$, is the expected value of $Y_i$ across values of $X_i$:

  - For continuous $Y_i$
  $$
  \E [Y_i \given X_i] = \int_{\mathcal{Y}} y f(y \given X_i) \, dy
  $$

  - For discrete $Y_i$:
  $$
  \E [Y_i \given X_i] = \sum_{\mathcal{Y}} y p(y \given X_i)
  $$
:::

. . .

- **Population-Level Function**: Describes the relationship between $Y_i$ and $X_i$ in the population (_finite_ or _super_).
- **Functional Flexibility**: Can be non-linear (!).

## Decomposition of Observed Outcomes

:::{.callout-important icon="false" title="CEF Decomposition Property"}
$$
Y_i = \underbrace{\E [Y_i \given X_i]}_{\text{explained by $X_i$}} + \underbrace{\varepsilon_i}_{\text{unexplained}},
$$

where $\E[\varepsilon_i \given X_i] = 0$ and $\varepsilon_i$ is uncorrelated with any function of $X_i$
:::

. . .

- [Intuition]{.note}:The CEF isolates the systematic component of $Y_i$ explained by $X_i$, while $\varepsilon_i$ captures noise.

. . .

- To see this property recall

  $$
  \begin{align*}
  \varepsilon_i &= Y_i - \E [Y_i \given X_i] \quad \implies\\
  \E [\varepsilon_i \given X_i] &= \E [Y_i - \E [Y_i \given X_i] \given X_i] = 0
  \end{align*}
  $$

- also $\E [h(X_i) \varepsilon_i] = 0$. Why?

## Best Minimal MSE Predictor

:::{.callout-important icon="false" title="CEF Prediction Property"}
$$
\E[Y_i \given X_i] = \argmin_{g(X_i)} \E \left[ (Y_i - g(X_i))^2 \right],
$$
where $g(X_i)$ is any function of $X_i$.
:::

- [Intuition]{.note}: CEF is the best method for predicting $Y_i$ in the least squares sense.

. . .

- To see this property decompose the squared expression:

$$
\begin{align*}
(Y_i - g(X_i))^2 &= \left(Y_i - \E [Y_i \given X_i] + \E [Y_i \given X_i] - g(X_i)\right)^2 \\
&= \left(Y_i - \E [Y_i \given X_i]\right)^2 + 2\left(Y_i - \E [Y_i \given X_i]\right)\left(\E [Y_i \given X_i] - g(X_i)\right) \\
&\quad + \left(\E [Y_i \given X_i] - g(X_i)\right)^2.
\end{align*}
$$

## Discrete Case  CEF

<br>

```{r}
#| label: cef_example
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

cef_df <- data.frame(
  X = rep(0:20, each = 100),
  Y = unlist(lapply(0:20, function(x) rnorm(100, mean = 6 + x * runif(1, 0.15, 0.25), sd = 0.5)))
)

# Summarize data for the CEF line
cef_line <- cef_df |>
  group_by(X) |>
  summarise(mean_Y = mean(Y))

# Create plot
ggplot(cef_df, aes(x = X, y = Y)) +
  ggdist::stat_halfeye(
    adjust = .33, ## bandwidth
    width = .67,
    color = NA, ## remove slab interval
    fill = "#b16286", ## remove slab interval
    position = position_nudge(x = .15),
    alpha = 0.8
  ) +
  geom_jitter(size = 1, alpha = 0.3, width = .1, height = 0, color = "#458588") + # Scatter of data points
  geom_line(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1) + # CEF line
  geom_point(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1.5) + # CEF line
  labs(
    x = "X",
    y = "Y"
  )

```

- The CEF is the _average line_ through the scatter of data points for each discrete $X$.
 
# Regression Justification

## Regression?

<br><br>

- The $\E [Y_i \given X_i]$ quantity looks very familiar, we already used in $\E [Y_i \given T_i]$ or $\E [Y_i \given T_i, X_i]$.

- We want to see if regression helps us with estimating these quantities. Especially when we want to estimate 

. . .

- [Note]{.note}: There is nothing _causal_ in $\E [Y_i \given T_i]$ or $\E [Y_i \given T_i, X_i]$, so we still need identification

  - We can for example rely on stong or conditional ignorability.

## Some Regression Coefficient Properties

- Before we move on to this, we need to recall important facts about regression coefficients

  :::incremental
  1. Population regression coefficients vector is given by (directly follows from $\E [X_i \varepsilon_i] = 0$) 
  $$
  \beta = \E [X_i X_i^{\prime}]^{-1} \E [X_i Y_i]
  $$

  2. Regression coefficient in single covariate case is given by
  $$
  \beta = \frac{\cov(Y_i,X_i)}{\var (X_i)} = \frac{\sum_{i = 1}^{n} (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i = 1}^{n} (X_i - \bar{X})^2}
  $$ 

  3. Regression coefficient in multiple covariate case is given by
  $$
  \beta_{k} = \frac{\cov(Y_i,\tilde{X}_{ki})}{\var (\tilde{X}_{ki})},
  $$
  where $\tilde{X}_{ki}$ is the vector of residuals from regression of $X_k$ on $X_{-k}$
  :::

## Justification 1: Linearity

<br>

:::{.callout-important icon="false" title="Theorem: Linear CEF"}
If CEF $\E [Y_i \given X_i]$ is linear in $X_i$, then the the population regression function, $X_i^{\prime} \beta$ returns exactly the $\E [Y_i \given X_i]$.
:::

. . .

- To see this property we can 

  - Use decomposition property of CEF to see $\E[ X_i (Y_i - \E [Y_i \given X_i]) ] = 0$

  - Substitute for $\E [Y_i \given X_i] = X_i^{\prime} b$ and solve

. . .

- How plausible is this result in practice?

. . .

- Always true in simple case is $\E [Y_i \given T_i] = \beta_0 + \beta_1 T_i$, where $T_i$ is binary treatment indicator.

## Binary Case CEF

<br>

```{r}
#| label: cef_binary
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

cef_bin_df <- data.frame(
  X = rep(0:1, each = 100),
  Y = unlist(lapply(0:1, function(x) rnorm(100, mean = 6 + x * runif(1, 0.15, 0.25), sd = 0.5)))
)

# Summarize data for the CEF line
cef_bin_line <- cef_bin_df |>
  group_by(X) |>
  summarise(mean_Y = mean(Y))

# Create plot
ggplot(cef_bin_df, aes(x = X, y = Y)) +
  geom_jitter(size = 1, alpha = 0.3, width = .05, height = 0, color = "#458588") +
  # same line
  stat_smooth(method = "lm", se = FALSE) +
  # same line
  geom_line(data = cef_bin_line, aes(x = X, y = mean_Y), color = "black", size = 1) +
  geom_point(data = cef_bin_line, aes(x = X, y = mean_Y), color = "black", size = 1.5) +
  scale_x_continuous(breaks = c(0, 1)) +
  labs(
    x = "T",
    y = "Y"
  )

```

## Justification 2: Linear Approximation

- What if the CEF is not linear?

. . .

- Regression can still be used to approximate the CEF:

:::{.callout-important icon="false" title="CEF Prediction Property"}
The function $X_i' \beta$ provides the Minimal MSE linear approximation to $\E [Y_i | X_i]$, that is:

$$
\beta = \argmin_b \E \left[ (\E [Y_i | X_i] - X_i' b)^2 \right].
$$
:::

. . .

- [Intuition]{.note}: Even if CEF is not linear we can use regression to approximate it and make substantive conclusions

. . .

- To see this we can decompose the squared error function minimized by OLS

$$
\begin{align*}
(Y_i - X_i' b)^2 &= \left( (Y_i - \E [Y_i | X_i]) + (\E [Y_i | X_i] - X_i' b) \right)^2 \\
&= (Y_i - \E [Y_i | X_i])^2 + (\E [Y_i | X_i] - X_i' b)^2 \\
&\quad + 2 (Y_i - \E [Y_i | X_i]) (\E [Y_i | X_i] - X_i' b).
\end{align*}
$$

  - The first term doesn't involve $b$.
  - The last term has an expectation of zero due to the CEF-decomposition property.

## Approximation of Discrete Case CEF

<br>

```{r}
#| label: cef_approximation_example
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Create plot
ggplot(cef_df, aes(x = X, y = Y)) +
  geom_jitter(size = 1, alpha = 0.3, width = .1, height = 0, color = "#458588") + # Scatter of data points
  stat_smooth(method = "lm", se = FALSE, color = "#cc241d") +
  geom_line(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1) + # CEF line
  geom_point(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1.5) + # CEF line
  labs(
    x = "X",
    y = "Y"
  )

```

## What Does This All Mean?

<br>

- In the case of CEF with respect to binary $X_i$ (think $T_i$), OLS provides estimate of $\E [Y_i \given X_i]$ which is the same as difference in means.

. . .

- In the case of CEF linear in $X_i$, OLS provides estimate of $\E [Y_i \given X_i]$ which is (constant) increase in means of $Y_i$.

. . .

- In the case of CEF non-linear in $X_i$, OLS provides best linear approximation of $\E [Y_i \given X_i]$

# Regression and Potential Outcomes

## Back to Simple Binary Setup

<br>

- Suppose $\mathcal{T} = \{0, 1\}$

- Under SUTVA (no interference and consistency) POs are $Y_{i} (1)$ and $Y_{i} (0)$.

- A unit-level treatment effect is, $\tau_i = Y_{i} (1) - Y_{i} (0)$

  - $\E [\tau_i] = \E [Y_{i} (1) - Y_{i} (0)] = \tau_{ATE}$ is the average treatment effect ($ATE$).

- We observe $X_i$, $T_i$ and, $Y_i = T_i Y_{i} (1) + (1 - T_i )Y_{i} (0)$.

. . .

- In this simple case OLS estimator solves the least squares problem:
  
  $$
  (\hat{\tau}, \hat{\alpha}) = \argmin_{\tau, \alpha} \sum_{i=1}^n \left(Y_i - \alpha - \tau T_i\right)^2
  $$

- Coefficient $\tau$ is mechanically the difference in means ($\tau_{DiM}$):
  
  $$
  \hat{\tau} = \bar{Y}_1 - \bar{Y}_0 = \hat{\tau}_{DiM}
  $$

## Regression Justification

- Key assumptions: **linearity** and **mean independence of errors**.  (why do we care about the latter?)

. . .

- Using switching equation we can show that:

$$
\begin{align*}
Y_i &= T_i Y_i(1) + (1 - T_i) Y_i(0) \\
&= Y_i(0) + T_i ( Y_i(1) - Y_i(0) ) \quad\text{($\because$ distribute)}\\
&= Y_i(0) + \tau_i T_i \quad \text{($\because$ unit treatment definition)}\\
&= \E [Y_i(0)] + \tau T_i + ( Y_i(0) - \E [Y_i(0)] ) + T_i (\tau_i - \tau) \quad (\because \pm \E [Y_i(0)] + \tau T_i)\\
&= \E [Y_i(0)] + \tau T_i + (1 - T_i)( Y_i(0) - \E [Y_i(0)] ) + T_i (Y_i(1) - \E [Y_i(1)]) \quad\text{($\because$ distribute)}\\
&= \alpha + \tau T_i + \epsilon_i
\end{align*}
$$

. . .

- _Linear_ functional form fully justified by SUTVA assumption alone:
  
  - [Intercept]{.highlight}: $\alpha = \E [Y_i(0)]$ (average control outcome).
  - [Slope]{.highlight}: $\tau = \E [Y_i(1) - Y_i(0)]$ (average treatment effect).
  - [Error]{.highlight}: deviation of control PO + treatment effect heterogeneity. What is the second interpretation?

## Mean independent errors

- The error is given by

$$
\varepsilon_i = (Y_i(0) - \E [Y_i(0)]) + T_i (\tau_i - \tau)
$$

- Does mean independence $\E [\varepsilon_i \given D_i] = 0$?

. . .

$$
\begin{align*}
\E [\varepsilon_i \given T_i] &= \E [(Y_i(0) - \E [Y_i(0)]) + T_i \cdot (\tau_i - \tau) \given T_i] \\
&= \E [Y_i(0) \given T_i] - \E [Y_i(0)] + T_i (\E [\tau_i \given T_i] - \tau)
\end{align*}
$$

. . .

- When would this be equal to zero? [E.g. under random assignment (strong ignorability)]{.fragment}

. . .

  $$
  \E [\varepsilon_i \given T_i] = \E [Y_i(0)] - \E [Y_i(0)] + T_i (\E [\tau_i] - \tau) = 0
  $$

- **Randomization + consistency allow linear model.**

  - Does not imply homoskedasticity or normal errors, though... Need to assume that!

# Regression with Covariates

## Linear Potential Outcomes

- Now assume constant linear treatment effects:  

$$
f_i(t) = \alpha + \tau t + \eta_i
$$  

. . .

- Observed outcomes are given by:  

$$
Y_i = f_i(T_i) = \alpha + \tau T_i + \eta_i,
$$  

where $\eta_i$ captures all variable determinants of $f_i(T_i)$ other than $T_i$.

. . .

- We also assume linear decomposition of $\eta_i$:  

$$
\eta_i = X_i^\prime \gamma + \nu_i,
$$  
  
where $\gamma$ is the population regression solution.

- Orthogonality of residuals to regressors in the population implies $\E [X_i \nu_i] = 0$.


## Linear Potential Outcomes

<br>

- We further assume linearity in $X_i$ as well (!)  

$$
\E [\eta_i | X_i] = X_i^\prime \gamma
$$  
  

. . .

- Under this model:  
    
$$
f_i(t) \indep T_i \given X_i \implies \nu_i \indep T_i \given X_i,
$$  
    
since when $X_i$ is fixed only $\nu_i$ is varying in $\eta_i$.

. . .

- With strong ignorability and linearity, we obtain:  

$$
Y_i = \alpha + \tau T_i + X_i^{\prime} \gamma + \nu_i,
$$  
    
where $\nu_i$ is uncorrelated with $X_i$ and also with $T_i$ conditional on $X_i$.

## From General Potential Outcomes to Regression

<br>

- By conditional ignorability,  

$$
\E [f_i(t) \given T_i = t, X_i] = \E [f_i(t) \given X_i] = \alpha + \tau t + X_i^{\prime} \gamma.
$$

. . .

- Then,

$$
\begin{align*}
\E [f_i(t) &- f_i(t - v) \given X_i] \\
&= (\alpha + \tau t + X_i^{\prime} \gamma) - (\alpha + \tau (t - v) + X_i^{\prime} \gamma) \\
&= \tau v
\end{align*}
$$

_($X_i$ disappeared because of the linearly separable confounding.)_

. . .

- [Result]{.note}:

  - So, $\tau$ is the **causal effect** of a unit change in $t$.
  - $\nu_i$ is uncorrelated with $X_i$ and $T_i$, so OLS is consistent for $\tau$.