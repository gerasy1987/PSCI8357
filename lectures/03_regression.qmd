---
title: "Regression?<br>Regression!"
subtitle: "PSCI 8357 - Stats II"
author: Georgiy Syunyaev
institute: "Department of Political Science, Vanderbilt University"
date: today
date-format: long
format: 
  revealjs:
    toc: true
    toc-depth: 1
    toc-title: "Plan"
    slide-number: c/t
    # preview-links: true
    embed-resources: true
    link-external-icon: true
    link-external-newwindow: true
    html-math-method: mathjax
    # logo: images/wzb_logo.png
    self-contained-math: true
    css: ../_supp/styles.css
    theme: [serif,"../_supp/custom.scss"]
    incremental: false
    self-contained: true
    citations-hover: true
    fragments: true
    # progress: true
    scrollable: false
    transition: fade
    reference-location: document
    fig-cap-location: top
    include-before: [ '<script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {enableAssistiveMml: false}});</script>']
fontsize: 26px
editor: source
aspectratio: 169
bibliography: ../_supp/psci8357.bib
---


## {data-visibility="hidden"}

\(
  \def\E{{\mathbb{E}}}
  \def\Pr{{\textrm{Pr}}}
  \def\var{{\mathbb{V}}}
  \def\cov{{\mathrm{cov}}}
  \def\corr{{\mathrm{corr}}}
  \def\argmin{{\arg\!\min}}
  \def\argmax{{\arg\!\max}}
  \def\qedknitr{{\hfill\rule{1.2ex}{1.2ex}}}
  \def\given{{\:\vert\:}}
  \def\indep{{\mbox{$\perp\!\!\!\perp$}}}
\)

```{r}
#|  label: preamble
#|  include: false

# load necessary libraries
pacman::p_load(tidyverse)

# set theme for plots
thematic::thematic_rmd(bg = "#f0f1eb", fg = "#111111", accent = "#111111")

get_bias <- function(tau = .5, delta = 0.3, gamma = 0.3) {
  # confounder
  confounder <- rnorm(n, mean = 50, sd = 10)

  # Democratic institutions (correlated with confounder)
  democracy_score <- delta * confounder + rnorm(n, mean = 0, sd = 5)

  # Economic growth (influenced by both investment and democratic institutions)
  growth <- tau * democracy_score + gamma * confounder + rnorm(n, mean = 0, sd = 5)

  # Regression ignoring the confounder
  model_biased <- lm(growth ~ democracy_score)
  # summary(model_biased)

  # True regression including the confounder
  model_unbiased <- lm(growth ~ democracy_score + confounder)
  # summary(model_unbiased)

  return(
    c(
      biased = unname(model_biased$coefficients[2] - tau),
      unbiased = unname(model_unbiased$coefficients[2] - tau)
    )
  )
}
```


## DiM vs. Regression

<br>

- So far we considered difference in means as our naive estimator of causal quantities.

. . .

- This week we will see, that we might use regression _agnostically_ to estimate causal estimands as well.

  - this makes our life easier, especially if we would like to rely on [conditional ignorability]{.highlight} assumption. (Why?)

. . .

- [BUT]{.note} this only solves the estimation problem.

  - We still have to make assumptions to achieve causal identification!

. . .

- [Problem]{.alert}: If we want to learn about relationship between $X$ and $Y$

  - The ideal is to learn about $f_{YX}(\cdot)$, 
  - In practice we learn about $\E [Y \given X]$.

# CEF

## Conditional Expectation Function (CEF)

:::{.callout-important icon="false" title="CEF"}

The [CEF]{.highlight}, $\E [Y_i \given X_i]$, is the expected value of $Y_i$ across values of $X_i$:

  - For continuous $Y_i$
  $$
  \E [Y_i \given X_i] = \int_{\mathcal{Y}} y f(y \given X_i) \, dy
  $$

  - For discrete $Y_i$:
  $$
  \E [Y_i \given X_i] = \sum_{\mathcal{Y}} y p(y \given X_i)
  $$
:::

. . .

- **Population-Level Function**: Describes the relationship between $Y_i$ and $X_i$ in the population (_finite_ or _super_).
- **Functional Flexibility**: Can be non-linear (!).

## Decomposition of Observed Outcomes

:::{.callout-important icon="false" title="CEF Decomposition Property"}
$$
Y_i = \underbrace{\E [Y_i \given X_i]}_{\text{explained by $X_i$}} + \underbrace{\varepsilon_i}_{\text{unexplained}},
$$

where $\E[\varepsilon_i \given X_i] = 0$ and $\varepsilon_i$ is uncorrelated with any function of $X_i$
:::

- [Intuition]{.note}:The CEF isolates the systematic component of $Y_i$ explained by $X_i$, while $\varepsilon_i$ captures noise.

. . .

- To see this property recall

  $$
  \begin{align*}
  \varepsilon_i &= Y_i - \E [Y_i \given X_i] \quad \implies\\
  \E [\varepsilon_i \given X_i] &= \E [Y_i - \E [Y_i \given X_i] \given X_i] = 0
  \end{align*}
  $$

- also $\E [h(X_i) \varepsilon_i] = 0$. (How can we use Law of Iterated Expectations to prove this?)

## Best Minimal MSE Predictor

:::{.callout-important icon="false" title="CEF Prediction Property"}
$$
\E[Y_i \given X_i] = \argmin_{g(X_i)} \E \left[ (Y_i - g(X_i))^2 \right],
$$
where $g(X_i)$ is any function of $X_i$.
:::

- [Intuition]{.note}: CEF is the best method for predicting $Y_i$ in the least squares sense.

. . .

- To see this property decompose the squared expression:

$$
\begin{align*}
(Y_i - g(X_i))^2 &= \left(Y_i - \E [Y_i \given X_i] + \E [Y_i \given X_i] - g(X_i)\right)^2 \\
&= \left(Y_i - \E [Y_i \given X_i]\right)^2 + 2\left(Y_i - \E [Y_i \given X_i]\right)\left(\E [Y_i \given X_i] - g(X_i)\right) \\
&\quad + \left(\E [Y_i \given X_i] - g(X_i)\right)^2.
\end{align*}
$$

## Discrete Case  CEF

<br>

```{r}
#| label: cef_example
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

cef_df <- data.frame(
  X = rep(0:20, each = 100),
  Y = unlist(lapply(0:20, function(x) rnorm(100, mean = 6 + x * runif(1, 0.15, 0.25), sd = 0.5)))
)

# Summarize data for the CEF line
cef_line <- cef_df |>
  group_by(X) |>
  summarise(mean_Y = mean(Y))

# Create plot
ggplot(cef_df, aes(x = X, y = Y)) +
  ggdist::stat_halfeye(
    adjust = .33, ## bandwidth
    width = .67,
    color = NA, ## remove slab interval
    fill = "#b16286", ## remove slab interval
    position = position_nudge(x = .15),
    alpha = 0.8
  ) +
  geom_jitter(size = 1, alpha = 0.3, width = .1, height = 0, color = "#458588") + # Scatter of data points
  geom_line(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1) + # CEF line
  geom_point(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1.5) + # CEF line
  labs(
    x = "X",
    y = "Y"
  )

```

- The CEF is the _average line_ through the scatter of data points for each discrete $X$.
 
# Regression Justification

## Regression?

<br><br>

- The $\E [Y_i \given X_i]$ quantity looks very familiar, we already used in $\E [Y_i \given T_i]$ or $\E [Y_i \given T_i, X_i]$.

- We want to see if regression helps us with estimating these quantities. Especially when we want to estimate 

. . .

- [Note]{.note}: There is nothing _causal_ in $\E [Y_i \given T_i]$ or $\E [Y_i \given T_i, X_i]$, so we still need identification

  - We can for example rely on stong or conditional ignorability.

## Some Regression Coefficient Properties

- Before we move on to this, we need to recall important facts about regression coefficients

  :::incremental
  1. Population regression coefficients vector is given by (directly follows from $\E [X_i \varepsilon_i] = 0$) 
  $$
  \beta = \E [X_i X_i^{\prime}]^{-1} \E [X_i Y_i]
  $$

  2. Regression coefficient in single covariate case is given by
  $$
  \beta = \frac{\cov(Y_i,X_i)}{\var (X_i)} = \frac{\sum_{i = 1}^{n} (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i = 1}^{n} (X_i - \bar{X})^2}
  $$ 

  3. Regression coefficient in multiple covariate case is given by
  $$
  \beta_{k} = \frac{\cov(\tilde{Y}_i,\tilde{X}_{ki})}{\var (\tilde{X}_{ki})},
  $$
  where $\tilde{X}_{ki}$ is the vector of residuals from regression of $X_k$ on $X_{-k}$
  :::

## Justification 1: Linearity

<br>

:::{.callout-important icon="false" title="Theorem: Linear CEF"}
If CEF $\E [Y_i \given X_i]$ is linear in $X_i$, then the the population regression function, $X_i^{\prime} \beta$ returns exactly the $\E [Y_i \given X_i]$.
:::

. . .

- To see this property we can 

  - Use decomposition property of CEF to see $\E[ X_i (Y_i - \E [Y_i \given X_i]) ] = 0$

  - Substitute for $\E [Y_i \given X_i] = X_i^{\prime} b$ and solve

. . .

- How plausible is this result in practice?

. . .

- Always true in simple case is $\E [Y_i \given T_i] = \beta_0 + \beta_1 T_i$, where $T_i$ is binary treatment indicator.

## Binary Case CEF

<br>

```{r}
#| label: cef_binary
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

cef_bin_df <- data.frame(
  X = rep(0:1, each = 100),
  Y = unlist(lapply(0:1, function(x) rnorm(100, mean = 6 + x * runif(1, 0.15, 0.25), sd = 0.5)))
)

# Summarize data for the CEF line
cef_bin_line <- cef_bin_df |>
  group_by(X) |>
  summarise(mean_Y = mean(Y))

# Create plot
ggplot(cef_bin_df, aes(x = X, y = Y)) +
  geom_jitter(size = 1, alpha = 0.3, width = .05, height = 0, color = "#458588") +
  # same line
  stat_smooth(method = "lm", se = FALSE) +
  # same line
  geom_line(data = cef_bin_line, aes(x = X, y = mean_Y), color = "black", size = 1) +
  geom_point(data = cef_bin_line, aes(x = X, y = mean_Y), color = "black", size = 1.5) +
  scale_x_continuous(breaks = c(0, 1)) +
  labs(
    x = "T",
    y = "Y"
  )

```

## Justification 2: Linear Approximation

- What if the CEF is not linear?

. . .

- Regression can still be used to approximate the CEF:

:::{.callout-important icon="false" title="CEF Prediction Property"}
The function $X_i' \beta$ provides the Minimal MSE linear approximation to $\E [Y_i | X_i]$, that is:

$$
\beta = \argmin_b \E \left[ (\E [Y_i | X_i] - X_i' b)^2 \right].
$$
:::

. . .

- [Intuition]{.note}: Even if CEF is not linear we can use regression to approximate it and make substantive conclusions

. . .

- To see this we can decompose the squared error function minimized by OLS

$$
\begin{align*}
(Y_i - X_i' b)^2 &= \left( (Y_i - \E [Y_i | X_i]) + (\E [Y_i | X_i] - X_i' b) \right)^2 \\
&= (Y_i - \E [Y_i | X_i])^2 + (\E [Y_i | X_i] - X_i' b)^2 \\
&\quad + 2 (Y_i - \E [Y_i | X_i]) (\E [Y_i | X_i] - X_i' b).
\end{align*}
$$

  - The first term doesn't involve $b$.
  - The last term has an expectation of zero due to the CEF-decomposition property.

## Approximation of Discrete Case CEF

<br>

```{r}
#| label: cef_approximation_example
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Create plot
ggplot(cef_df, aes(x = X, y = Y)) +
  geom_jitter(size = 1, alpha = 0.3, width = .1, height = 0, color = "#458588") + # Scatter of data points
  stat_smooth(method = "lm", se = FALSE, color = "#cc241d") +
  geom_line(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1) + # CEF line
  geom_point(data = cef_line, aes(x = X, y = mean_Y), color = "black", size = 1.5) + # CEF line
  labs(
    x = "X",
    y = "Y"
  )

```

## What Does This All Mean?

<br>

- In the case of CEF with respect to binary $X_i$ (think $T_i$), OLS provides estimate of $\E [Y_i \given X_i]$ which is the same as difference in means.

. . .

- In the case of CEF linear in $X_i$, OLS provides estimate of $\E [Y_i \given X_i]$ which is (constant) increase in means of $Y_i$.

. . .

- In the case of CEF non-linear in $X_i$, OLS provides best linear approximation of $\E [Y_i \given X_i]$

# Regression and Potential Outcomes

## Back to Simple Binary Setup

<br>

- Suppose $\mathcal{T} = \{0, 1\}$

- Under SUTVA (no interference and consistency) POs are $Y_{i} (1)$ and $Y_{i} (0)$.

- A unit-level treatment effect is, $\tau_i = Y_{i} (1) - Y_{i} (0)$

  - $\E [\tau_i] = \E [Y_{i} (1) - Y_{i} (0)] = \tau_{ATE}$ is the average treatment effect ($ATE$).

- We observe $X_i$, $T_i$ and, $Y_i = T_i Y_{i} (1) + (1 - T_i )Y_{i} (0)$.

. . .

- In this simple case OLS estimator solves the least squares problem:
  
  $$
  (\hat{\tau}, \hat{\alpha}) = \argmin_{\tau, \alpha} \sum_{i=1}^n \left(Y_i - \alpha - \tau T_i\right)^2
  $$

- Coefficient $\tau$ is mechanically the difference in means ($\tau_{DiM}$):
  
  $$
  \hat{\tau} = \bar{Y}_1 - \bar{Y}_0 = \hat{\tau}_{DiM}
  $$

## Regression Justification

- Key assumptions: **linearity** and **mean independence of errors**.  (why do we care about the latter?)

. . .

- Using switching equation we can show that:

$$
\begin{align*}
Y_i &= T_i Y_i(1) + (1 - T_i) Y_i(0) \\
&= Y_i(0) + T_i ( Y_i(1) - Y_i(0) ) \quad\text{($\because$ distribute)}\\
&= Y_i(0) + \tau_i T_i \quad \text{($\because$ unit treatment definition)}\\
&= \E [Y_i(0)] + \tau T_i + ( Y_i(0) - \E [Y_i(0)] ) + T_i (\tau_i - \tau) \quad (\because \pm \E [Y_i(0)] + \tau T_i)\\
&= \E [Y_i(0)] + \tau T_i + (1 - T_i)( Y_i(0) - \E [Y_i(0)] ) + T_i (Y_i(1) - \E [Y_i(1)]) \quad\text{($\because$ distribute)}\\
&= \alpha + \tau T_i + \eta_i
\end{align*}
$$

. . .

- _Linear_ functional form fully justified by SUTVA assumption alone:
  
  - [Intercept]{.highlight}: $\alpha = \E [Y_i(0)]$ (average control outcome).
  - [Slope]{.highlight}: $\tau = \E [Y_i(1) - Y_i(0)]$ (average treatment effect).
  - [Error]{.highlight}: deviation of control PO + treatment effect heterogeneity. What is the second interpretation?

## Mean independent errors

- The error is given by

$$
\eta_i = (1 - T_i)( Y_i(0) - \E [Y_i(0)] ) + T_i (Y_i(1) - \E [Y_i(1)])
$$

- In regression context we would like $\E [\eta_i \given T_i] = 0$?

. . .

$$
\begin{align*}
\E [\eta_i \given T_i] &= \E [(1 - T_i)( Y_i(0) - \E [Y_i(0)] ) + T_i (Y_i(1) - \E [Y_i(1)]) \given T_i] \\
&= (1 - T_i) (\E [Y_i(0) \given T_i] - \E [Y_i(0)]) + T_i (\E [Y_i(1) \given T_i] - \E [Y_i(1)])
\end{align*}
$$

- Does this look familiar? [This is selection wrt to $Y_i(0)$ and $Y_i(1)$.]{.fragment}

. . .

- When would this be equal to zero? [E.g. under random assignment (strong ignorability)]{.fragment}

. . .

  $$
  \E [\eta_i \given T_i] = (1 - T_i) (\E [Y_i(0)] - \E [Y_i(0)]) + T_i (\E [Y_i(1)] - \E [Y_i(1)]) = 0
  $$

- **Randomization + consistency allow linear model.**

  - Does not imply homoskedasticity or normal errors, though... Need to assume that!

# Regression with Covariates

## Selection on Observables

<br>

- Under strong ignorability we can use regression to estimate causal effects of interest.

- What if instead we assume that selection depends on a set of observed covariates $X_{i}$, i.e. there is [selection on observables]{.highlight}?

. . .

- This implies the [conditional ignorability]{.highlight} assumption, i.e.

$$
\{ Y_i(0), Y_i(1) \} \indep T_i \given X_i. 
$$

. . .

- This in turn implies $\eta_i \indep T_i \given X_i$ (Why?)

. . .

- [Note]{.note}: If conditional ignorability is true and we are able to condition on $X_i$, the following is also true

$$
\eta_i \indep T_i  \given X_i \implies \E [\eta_i \given T_i=1, X_i] = \E [\eta_i \given T_i=0, X_i] = \E [\eta_i \given X_i]. 
$$

## Linear POs

- Now assume constant linear treatment effects

$$
f_i(t) = \alpha + \tau t + \eta_i
$$  

. . .

- Observed outcomes are given by:  

  $$
  Y_i = f_i(T_i) = \alpha + \tau T_i + \eta_i,
  $$  

  where $\eta_i$ captures all variable determinants of $f_i(T_i)$ other than $T_i$.

. . .

- We also assume linear decomposition of $\eta_i$:  

  $$
  \eta_i = X_i^{\prime} \gamma + \nu_i,
  $$
  
  
  where $\gamma$ is the population regression solution.

- Orthogonality of residuals to regressors in the population implies $\E [X_i \nu_i] = 0$.


## Linear POs

<br>

- We further assume linearity in $X_i$ as well (!)  

$$
\E [\eta_i | X_i] = X_i^\prime \gamma
$$  
  

. . .

- Under this model we have
    
  $$
  f_i(t) \indep T_i \given X_i \implies \nu_i \indep T_i \given X_i,
  $$  
    
  since when $X_i$ is fixed only $\nu_i$ is varying in $\eta_i$.

. . .

- With strong ignorability and linearity, we obtain:  

  $$
  Y_i = \alpha + \tau T_i + X_i^{\prime} \gamma + \nu_i,
  $$ 

  where $\nu_i$ is uncorrelated with $X_i$ and also with $T_i$ conditional on $X_i$.

## From Linear POs to Regression

<br>

- By conditional ignorability,  

$$
\E [f_i(t) \given T_i = t, X_i] = \E [f_i(t) \given X_i] = \alpha + \tau t + X_i^{\prime} \gamma.
$$

. . .

- Then,

$$
\begin{align*}
\E [f_i(t) &- f_i(t - v) \given X_i] \\
&= (\alpha + \tau t + X_i^{\prime} \gamma) - (\alpha + \tau (t - v) + X_i^{\prime} \gamma) \\
&= \tau v
\end{align*}
$$

- _($X_i$ disappeared because of the linearly separable confounding.)_

. . .

- [Result]{.note}:

  - So, $\tau$ is the **causal effect** of a unit change in $t$.
  - $\nu_i$ is uncorrelated with $X_i$ and $T_i$, so OLS is consistent for $\tau$.

# Omitted Variable Bias

## Omitted Variable Bias (OVB) {#ovb}

- Now suppose we _erroneously_ omit $X_i$, and just regress $Y_i$ on $T_i$ via OLS.  

. . .

- To see [omitted variable bias]{.highlight} we look at what the coefficient on $T_i$ estimates, $\frac{\cov(Y_i, T_i)}{\var(T_i)}$ assuming that the true model should include $X_i$:

$$
\begin{align*}
\cov(Y_i, T_i) &= \cov(\alpha + \tau T_i + X_i' \gamma + \nu_i,\, T_i) \\
&= \tau \cov(T_i, T_i) + \cov(X_{1i} \gamma_1 + \ldots + X_{Ki} \gamma_K, T_i) \\
&= \tau \var (T_i) + \gamma_1 \cov(X_{1i}, T_i) + \ldots + \gamma_K \cov(X_{Ki}, T_i)
\end{align*}
$$  

. . .

$$
\implies \frac{\cov(Y_i, T_i)}{\var (T_i)} = \tau + \underbrace{\gamma^{\prime} \delta}_{\text{OVB}}
$$

where $\delta$ are coefficients from regressions of $X_1, \ldots, X_K$ on $T_i$.

. . .

- By the [Frisch–Waugh–Lovell theorem](#fwl-theorem), if we include some of $X_i$ we will get $\frac{\cov(\tilde{Y}_i, \tilde{T}_i)}{\var (\tilde{T}_i)} = \tau + \tilde{\gamma}^{\prime}\tilde{\delta}$, where $\tilde{\cdot}$ means residualized with respect to included terms from $X_i$.

## Omitted Variable Bias

<br>

- [OVB]{.highlight} = $\gamma^\prime \delta$, where

  - $\gamma$ is the vector of effects of [confounders]{.highlight} on the outcome.
  - $\delta$ is the vector of effects of [treatment on the confounders]{.highlight} —- i.e., the degree of confounder-variable imbalance.

. . .

- Same holds when we consider the case where we include _some_ controls: 
  
  $$
  \text{OVB} = \tilde{\gamma}' \tilde{\delta}.
  $$
  
  Everything is just defined in terms of variables that have been residualized with respect to the included controls.

- OVB = [confounder impact $\times$ imbalance]{.highlight} [@cinelli2020making].

## Omitted Variable Bias


- Let's practice applying the OVB formula:

  OVB = $(X_{ki}, Y_i)$ relationships $\times$ $(X_{ki}, T_i)$ relationships

```{dot}
//| fig-width: 4
//| fig-height: 2
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="LR"
  newrank=true;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=3,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    shape=plaintext,
    fontsize=24,
    penwidth=2
  ]
  Y;
  T->Y;

  subgraph U {
    node [
      shape=plaintext
      fontcolor="#cc241d"
      fontsize=24
      penwidth=2
    ]
		edge [color = "#cc241d"]
  X->Y;X->T;
}
}
```

. . .

1.	Effect of democratic institutions on growth, estimated via regression of growth on democratic institutions.

2.	Effect of exposure to negative advertisements on turnout, estimated via regression of turnout on the number of ads seen.

- [Question]{.note}: What is a possible omitted variable? How will this bias the estimate?

## OVB: Simulate DAG Relationship

```{r}
#| echo: true
#| eval: true
#| output: asis

set.seed(20250127) # set seed

n <- 1000 # sample size
tau <- 0.5 # ATE
gamma <- 0.3 # effect of confounder on outcome
delta <- 0.3 # proportional to effect of treatment on confounder (only if less than sd ratio)

# confounder
confounder <- rnorm(n, mean = 50, sd = 10)

# democratic institutions (correlated with confounder)
democracy_score <- delta * confounder + rnorm(n, mean = 0, sd = 5)

# economic growth (influenced by both investment and democratic institutions)
growth <- tau * democracy_score + gamma * confounder + rnorm(n, mean = 0, sd = 5)

# regression ignoring the confounder
model_biased <- lm(growth ~ democracy_score)

# true regression including the confounder
model_unbiased <- lm(growth ~ democracy_score + confounder)

cat(unname(model_unbiased$coefficients[2]) - 0.5) # error in unbiased model
cat(unname(model_biased$coefficients[2]) - 0.5) # error in biased model
```

## OVB: High $\gamma$, High $\delta$

```{r}
#| label: ovb_hg_hd
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  replicate(1000, get_bias(delta = 0.5, gamma = 0.5)) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(cols = c(Biased, Unbiased), names_to = "Type", values_to = "Value")

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed", size = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  ) +
  theme_minimal()
```

## OVB: High $\gamma$, Low $\delta$

```{r}
#| label: ovb_hg_ld
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  replicate(1000, get_bias(delta = 0.1, gamma = 0.5)) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(cols = c(Biased, Unbiased), names_to = "Type", values_to = "Value")

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed", size = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  ) +
  theme_minimal()
```

## OVB: Low $\gamma$, High $\delta$

```{r}
#| label: ovb_lg_hd
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  replicate(1000, get_bias(delta = 0.5, gamma = 0.1)) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(cols = c(Biased, Unbiased), names_to = "Type", values_to = "Value")

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed", size = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  ) +
  theme_minimal()
```

## OVB: Low $\gamma$, Low $\delta$

```{r}
#| label: ovb_lg_ld
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  replicate(1000, get_bias(delta = 0.1, gamma = 0.1)) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(cols = c(Biased, Unbiased), names_to = "Type", values_to = "Value")

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed", size = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  ) +
  theme_minimal()
```


## Be Careful!

<br><br>

- **Omitted variables** is a misleading term because it could suggest that you want to include *any* variable that is correlated with treatment and outcomes.

- But remember [bad controls]{.highlight} exist:

  - Common descendants of treatment and outcome
  - $M$-bias variables
  - We will look at practice next

## Two Ways to Adjust for Covariates

<br>

- The discussion of OVB suggests that we can use regression to adjust for variables ($X_i$) to estimate the treatment effect ($\tau$) in two ways.

  1. [_Long_ regression]{.highlight}: Include covariates $X_i$ directly in the regression model.

  2. [Residualized regression]{.highlight}:
     a. Purge variation in $Y_i$ due to $X_i$ $\rightarrow$ Regress $Y_i$ on $X_i$ and calculate residual outcomes: $\tilde{Y}_i = Y_i - \hat{Y}_i$.
     b. Purge variation in $T_i$ due to $X_i$ $\rightarrow$ Regress $T_i$ on $X_i$ and calculate residual treatments: $\tilde{T}_i = T_i - \hat{T}_i$.
     c. Regress $\tilde{Y}_i$ on $\tilde{T}_i$.

- [Result]{.note}: Coefficient on $T_{i}$ in _long_ regression and on $\tilde{T}_i$ in residualized regression are identical.

# Back-Door Criterion

## Identification Analysis with Causal Graphs

- An alternative, perhaps more intuitive, way to think about confounding is in terms of DAGs.
- Suppose we want to estimate the $ATE$ of $T$ on $Y$; which covariates do we need to measure?

. . .

- Pearl develops criteria, which can be directly read off the graph alone.
- Before studying the criteria, we need to define some new concepts.

. . .

:::{.columns}
::: {.column width="70%"}

- [Nodes]{.highlight}: $T$, $Y$, $Z_1$, $Z_2$, and $Z_3$.

- [Paths]{.highlight}: $T \to Y$, $T \leftarrow Z_3 \to Y$, $T \leftarrow Z_1 \to Z_3 \leftarrow Z_2 \to Y$, etc.

- $Z_1$ is a [parent]{.highlight} of $T$ and $Z_3$. 

- $T$ and $Z_3$ are [children]{.highlight} of $Z_1$.

- $Z_1$ is an [ancestor]{.highlight} of $Y$. 

- $Y$ is a [descendant]{.highlight} of $Z_1$.

:::

::: {.column width="30%"}
```{dot}
//| fig-width: 3
//| fig-height: 3
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    // Nodes
    T;
    Y;
    Z_1;
    Z_2;
    Z_3;

    // Edges
    Z_1 -> T;
    Z_1 -> Z_3;
    Z_2 -> Y;
    Z_2 -> Z_3;
    Z_3 -> T;
    Z_3 -> Y;
    T -> Y;
    {{ rank=min; Z_1;Z_2};}
    {{ rank=max; T;Y};}
}
```
:::
:::
  

## Blocked Paths and $d$-separation

:::{.columns}
::: {.column width="65%"}
::: {.callout-important icon="false" title="Definition: Blocked Paths"}

A set of nodes $S$ **blocks** a path $p$ if either:

1. $p$ contains at least one *arrow-emitting node* in $S$, OR
2. $p$ contains at least one *collision node* that is outside $S$ and has no descendant in $S$.
:::

:::{.fragment}
- $T \leftarrow W_1 \leftarrow Z_1 \to Z_3 \to Y$ is blocked by $\{W_1\}$, $\{Z_1\}$, $\{Z_1, Z_3\}$, etc.
- $T \leftarrow W_1 \leftarrow Z_1 \to Z_3 \leftarrow Z_2 \to W_2 \to Y$ is blocked by $\{\emptyset\}$ (an empty set).
:::

:::

::: {.column width="35%"}
```{dot}
//| fig-width: 3
//| fig-height: 3
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    // Nodes
    T;
    Y;
    Z_1;
    Z_2;
    Z_3;

    // Edges
    Z_1 -> W_1 -> T;
    Z_1 -> Z_3;
    Z_2 -> W_2 -> Y;
    Z_2 -> Z_3;
    Z_3 -> T;
    Z_3 -> Y;
    T -> W_3 -> Y;
    {{ rank=min; Z_1;Z_2};}
    {{ rank=same; W_1;Z_3;W_2};}
    {{ rank=max; T;W_3;Y};}
}
```
:::
:::

. . .

::: {.callout-important icon="false" title="Definition: $d$-separation"}
If $S$ blocks all paths from $T$ to $Y$, then $S$ [$d$-separates]{.highlight} $T$ and $Y$.  
If $S$ $d$-separates $T$ and $Y$, then $Y \indep T \given S$.
:::

- $W_1$ and $Z_3$ are $d$-separated by set $S = \{Z_1\}$.

## The Back-Door Criterion for Causal Identification

::: {.callout-important icon="false" title="Theorem: The Back-Door Criterion"}
A set $S$ is sufficient for adjustment to identify the causal effect of $T$ on $Y$ if:
  
  1. No element of $S$ is a descendant of $T$, **and**
  2. The elements of $S$ block all [back-door paths]{.highlight} from $T$ to $Y$.
:::

- [Important]{.note}: The [back-door criterion]{.highlight} tells you which covariates to condition on to identify a causal effect, given a hypothesized DAG.

. . .

:::{.columns}
::: {.column width="40%"}
```{dot}
//| fig-width: 4
//| fig-height: 3
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    // Nodes
    T;
    Y;
    Z_1;
    Z_2;
    Z_3;

    // Edges
    Z_1 -> W_1 -> T;
    Z_1 -> Z_3;
    Z_2 -> W_2 -> Y;
    Z_2 -> Z_3;
    Z_3 -> T;
    Z_3 -> Y;
    T -> W_3 -> Y;
    {{ rank=min; Z_1;Z_2};}
    {{ rank=same; W_1;Z_3;W_2};}
    {{ rank=max; T;W_3;Y};}
}
```
:::

::: {.column width="60%"}

<br>

- [Example]{.note}: Which variables should we control for to identify the effect of $T$ on $Y$?
  - $S = \{W_1, W_2\}$? [**No.**]{.fragment}
  - $S = \{Z_1, Z_3\}$? [**Yes!**]{.fragment}
  - $S = \{Z_3\}$? [**No**, because it [unblocks]{.highlight} $T \leftarrow W_1 \leftarrow Z_1 \to Z_3 \leftarrow Z_2 \to W_2 \to Y$.]{.fragment}
:::
:::

# _The Good, The Bad, The Ugly..._ Controls

## Good Controls 1

:::{.columns}
::: {.column  width="50%"}


- [confounder]{.highlight} is a common cause of main explanatory variable, $X$, and outcome of interest, $Y$.

:::fragment
- **Model (a)** 
  - $Z$ is a common cause of $X$ and $Y$.
  - Controlling for $Z$ blocks the back-door path.
:::

:::fragment
- **Models (b) and (c):** 
  - $Z$ is not a common cause.
  - Still, controlling for $Z$ blocks the back-door path due to unobserved confounder $U$.
:::

:::
::: {.column  width="50%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]
  
  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_c [style=dashed, label="U"];
    
    U_c -> X_c -> Y_c;
    U_c -> Z_c -> Y_c;
    { rank=min; Z_c}
    { rank=max; X_c;Y_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_b [style=dashed, label="U"];
    
    U_b -> Z_b -> X_b -> Y_b;
    U_b -> Y_b;
    { rank=min; Z_b}
    { rank=max; X_b;Y_b}
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;

    Z [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    Z -> X -> Y;
    Z -> Y;
    { rank=min; Z}
    { rank=max; X;Y}
  }

}
```
:::
:::

## Good Controls 2

:::{.columns}
::: {.column  width="55%"}

<br><br>

- [Intuition]{.note}: Common causes of $X$ and any mediator $M$ (between $X$ and $Y$) also
confound the effect of $X$ on $Y$. 

:::fragment
- Models (a)-(c) are analogous to the models without mediator -- controlling for $Z$ blocks the back-door path from $X$ to $Y$ and produces an unbiased estimate of the $ATE$.
:::

:::
::: {.column  width="45%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]
  
  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    M_c [label="M"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_c [style=dashed, label="U"];
    
    U_c -> X_c -> M_c -> Y_c;
    U_c -> Z_c -> M_c;
    { rank=min; Z_c}
    { rank=same; X_c;M_c}
    { rank=max; Y_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    M_b [label="M"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_b [style=dashed, label="U"];
    
    U_b -> Z_b -> X_b -> M_b -> Y_b;
    U_b -> M_b;
    { rank=min; Z_b}
    { rank=same; X_b;M_b}
    { rank=max; Y_b}
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;

    Z [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    Z -> X -> M -> Y;
    Z -> M;
    { rank=min; Z}
    { rank=same; X;M}
    { rank=max; Y}
  }

}
```

:::
:::

## Neutral (?) Controls

:::{.columns}
::: {.column  width="55%"}

<br><br>

- [Intuition]{.note}: Ancestors of only $X$, only $Y$, or only $M$ (mediator) do not introduce bias. Controling for these factors will reduce variation in respective variable that is _not_ related to the variation in other variable.

:::fragment
- In model (a) reduction in variation is good! $\rightarrow$ higher precision

- In model (b) reduction in variation is bad! $\rightarrow$ lower precision

- In model (c) reduction in variation is good again! $\rightarrow$ higher precision
:::

:::
::: {.column  width="45%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]

  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    M_c [label="M"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    X_c -> M_c -> Y_c;
    Z_c -> M_c;
    { rank=min; X_c}
    { rank=same; Z_c;M_c}
    { rank=max; Y_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    Z_b -> X_b -> Y_b;
    {rank=same; Z_b;X_b};
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;
    Z [shape=circle, style=filled, fillcolor=red];
    X;
    Y;
    
    Z -> Y;
    X -> Y;
    {rank=same; Z;Y};
  }
}
```

:::
:::

## (More) Bad Controls

:::{.columns}
::: {.column  width="55%"}

<br><br>

- We already discussed collider and M-bias before, here are a few more

- [Intuition]{.note}: We do not want to block the _channels_ through which the effect goes (unless we are interested in $CATE$).

:::fragment
- In model (a) and (b) controlling for $Z$ blocks the causal path.

- In model (c) controlling for $Z$ will unblock the back-door path $X \rightarrow Z \leftarrow U \rightarrow Y$.
:::

:::
::: {.column  width="45%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]

  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_c [style=dashed, label="U"];
    
    X_c -> Z_c -> Y_c;
    U_c -> Z_c;
    U_c -> Y_c;
    { rank=min; X_c}
    { rank=same; Z_c}
    { rank=max; Y_c;U_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    M_b [label="M"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    X_b -> M_b -> Y_b;
    M_b -> Z_b
    {rank=same; M_b;Z_b};
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;
    Z [shape=circle, style=filled, fillcolor=red];
    X;
    Y;
    
    X -> Z -> Y;
  }
}
```

:::
:::

## Controls Conclusion

<br><br>

- Be mindful of what controls you include in your analysis (even if it is an experiment).

- Draw a DAG with controls you plan to include and see whether
  
  - You need them to _block_ any paths.
  - They might be colliders or introduce post-treatment bias.
  - Do not use "kitchen sink" approach!

- Be also mindful of the sizes of the effects of potential confounders. If the effect on main independent and dependent variable can be proven to be limited, the OVB is small!

# Appendix

## Frisch-Waugh-Lovell Theorem [🔙](#ovb) {#fwl-theorem visibility="uncounted"}

<br>

- Consider a multiple regression model: $Y_i = \alpha + \tau T_i + X_i^\prime \beta + \nu_i$.

- To find $\tau$, the coefficient on $T_i$, the [Frisch-Waugh-Lovell Theorem]{.highlight} states that:

  1. Regress $Y_i$ on $X_i$ and obtain the residuals $\tilde{Y}_i = Y_i - X_i^\prime \hat{\beta}$.
  
  2. Regress $T_i$ on $X_i$ and obtain the residuals $\tilde{T}_i = T_i - X_i^\prime \hat{\delta}$.
  
  3. Regress $\tilde{Y}_i$ on $\tilde{T}_i$ to obtain $\beta_1$.

  In addition, the $R^2$ and F-statistics of these regressions will be the same as those from the full model regression. 

- [Intuition]{.note}:
  - The Frisch-Waugh-Lovell theorem decomposes the estimation process.
  - Adjusts $Y_i$ and $T_i$ for covariates $X_i$ separately, highlighting the direct effect of $T_i$.

## References