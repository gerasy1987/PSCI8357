---
title: "Regression?<br>Regression!"
subtitle: "PSCI 8357 - STAT II"
author: Georgiy Syunyaev
institute: "Department of Political Science, Vanderbilt University"
date: today
date-format: long
format: 
  revealjs:
    toc: true
    toc-depth: 1
    toc-title: "Plan"
    slide-number: c/t
    # preview-links: true
    embed-resources: true
    link-external-icon: true
    link-external-newwindow: true
    html-math-method: mathjax
    # logo: images/wzb_logo.png
    self-contained-math: true
    css: ../_supp/styles.css
    theme: [serif,"../_supp/custom.scss"]
    incremental: false
    self-contained: true
    citations-hover: true
    fragments: true
    # progress: true
    scrollable: false
    transition: fade
    reference-location: document
    fig-cap-location: top
    include-in-header:
      - text: |
          <script>
          window.MathJax = {
            options: {
              enableAssistiveMml: false
            }
          };
          </script>
fontsize: 26px
editor: source
aspectratio: 169
bibliography: ../_supp/psci8357.bib
---


## {data-visibility="hidden"}

\(
  \def\E{{\mathbb{E}}}
  \def\Pr{{\textrm{Pr}}}
  \def\var{{\mathbb{V}}}
  \def\cov{{\mathrm{cov}}}
  \def\corr{{\mathrm{corr}}}
  \def\argmin{{\arg\!\min}}
  \def\argmax{{\arg\!\max}}
  \def\qedknitr{{\hfill\rule{1.2ex}{1.2ex}}}
  \def\given{{\:\vert\:}}
  \def\indep{{\mbox{$\perp\!\!\!\perp$}}}
  \def\notindep{{\mbox{$\centernot{\perp\!\!\!\perp}$}}}
\)

```{r}
#|  label: preamble
#|  include: false

# load necessary libraries
pacman::p_load(tidyverse, future, future.apply, pbapply)

future::plan(multisession, workers = max(1, parallel::detectCores() - 4))

# set theme for plots
thematic::thematic_rmd(bg = "#f0f1eb", fg = "#111111", accent = "#111111")

get_bias <- function(tau = .5, delta = 0.3, gamma = 0.3, n = 1000) {
  # confounder
  confounder <- rnorm(n, mean = 50, sd = 10)

  # Democratic institutions (correlated with confounder)
  democracy_score <- delta * confounder + rnorm(n, mean = 0, sd = 5)

  # Economic growth (influenced by both investment and democratic institutions)
  growth <- tau *
    democracy_score +
    gamma * confounder +
    rnorm(n, mean = 0, sd = 5)

  # Regression ignoring the confounder
  model_biased <- lm(growth ~ democracy_score)
  # summary(model_biased)

  # True regression including the confounder
  model_unbiased <- lm(growth ~ democracy_score + confounder)
  # summary(model_unbiased)

  return(
    c(
      biased = unname(model_biased$coefficients[2] - tau),
      unbiased = unname(model_unbiased$coefficients[2] - tau)
    )
  )
}

get_bias_het <- function(
    tau_base = .5,
    assignment_fun = function(x) .5 + 0.01 * x,
    het_fun = function(x) tau_base + 0.01 * x) {
  n <- 1000 # sample size
  gamma <- 0.1 # effect of X on outcome

  X <- sample(x = 1:100, replace = T, size = n) # some covariate
  tau_total <- sum(sapply(1:100, het_fun) / 100) # total treatment effect (assuming heterogeneity)

  # democratic institutions (correlated with confounder)
  democracy_high <- rbinom(n, size = 1, prob = .5)
  democracy_high_2 <-
    rbinom(n, size = 1, prob = sapply(X, assignment_fun))

  # economic growth (influenced by both investment and democratic institutions)
  growth <-
    het_fun(X) * democracy_high + gamma * X + rnorm(n, mean = 0, sd = 5)

  growth_2 <-
    het_fun(X) * democracy_high_2 + gamma * X + rnorm(n, mean = 0, sd = 5)

  # regression ignoring the confounder
  bias1 <- lm(growth ~ democracy_high + factor(X))$coefficients[2] - tau_total

  # regression ignoring the confounder
  bias2 <- lm(growth_2 ~ democracy_high_2 + factor(X))$coefficients[2] -
    tau_total

  return(c(het1 = unname(bias1), het2 = unname(bias2)))
}
```


## DiM vs. Regression

<br>

- So far we considered the difference in means as our naive estimator of causal quantities.

. . .

- This week we will see that we might use regression _agnostically_ to estimate causal estimands as well.

  - this makes our life easier, especially if we would like to rely on the [conditional ignorability]{.highlight} assumption. (Why?)

. . .

- [BUT]{.note} this only solves the estimation problem.

  - We still have to make assumptions to achieve causal identification!

. . .

- [Problem]{.alert}: If we want to learn about the relationship between $X$ and $Y$

  - The ideal is to learn about $f_{YX}(\cdot)$, 
  - In practice we learn about $\E [Y \given X]$.

# CEF

## Conditional Expectation Function (CEF)

:::{.callout-important icon="false" title="CEF"}

The [CEF]{.highlight}, $\E [Y_i \given X_i]$, is the expected value of $Y_i$ given (conditional on) $X_i$:

  - For continuous $Y_i$
  $$
  \E [Y_i \given X_i] = \int_{\mathcal{Y}} y f(y \given X_i) \, dy
  $$

  - For discrete $Y_i$:
  $$
  \E [Y_i \given X_i] = \sum_{\mathcal{Y}} y p(y \given X_i)
  $$
:::

. . .

- **Population-Level Function**: Describes the relationship between $Y_i$ and $X_i$ in the population (_finite_ or _super_).
- **Functional Flexibility**: Can be non-linear (!).

## Decomposition of Observed Outcomes

:::{.callout-important icon="false" title="CEF Decomposition Property"}
$$
Y_i = \underbrace{\E [Y_i \given X_i]}_{\text{explained by $X_i$}} + \underbrace{\varepsilon_i}_{\text{unexplained}},
$$

where $\E[\varepsilon_i \given X_i] = 0$ and $\varepsilon_i$ is uncorrelated with any function of $X_i$
:::

- [Intuition]{.note}: The CEF isolates the systematic component of $Y_i$ explained by $X_i$, while $\varepsilon_i$ captures noise.

. . .

- To see this property recall

  $$
  \begin{align*}
  \varepsilon_i &= Y_i - \E [Y_i \given X_i] \quad \implies\\
  \E [\varepsilon_i \given X_i] &= \E [Y_i - \E [Y_i \given X_i] \given X_i] = 0
  \end{align*}
  $$

- also $\E [h(X_i) \varepsilon_i] = 0$. (How can we use Law of Iterated Expectations to prove this?)

## Best Minimal MSE Predictor

:::{.callout-important icon="false" title="CEF Prediction Property"}
$$
\E[Y_i \given X_i] = \argmin_{g(X_i)} \E \left[ (Y_i - g(X_i))^2 \right],
$$
where $g(X_i)$ is any function of $X_i$.
:::

- [Intuition]{.note}: CEF is the best method for predicting $Y_i$ in the least squares sense.

. . .

- To see this property decompose the squared expression:

$$
\begin{align*}
(Y_i - g(X_i))^2 &= \left(Y_i - \E [Y_i \given X_i] + \E [Y_i \given X_i] - g(X_i)\right)^2 \\
&= \left(Y_i - \E [Y_i \given X_i]\right)^2 + 2\left(Y_i - \E [Y_i \given X_i]\right)\left(\E [Y_i \given X_i] - g(X_i)\right) \\
&\quad + \left(\E [Y_i \given X_i] - g(X_i)\right)^2.
\end{align*}
$$

## Discrete Case CEF

<br>

```{r}
#| label: cef_example
#| fig-align: center
#| fig-width: 8
#| fig-height: 5
#| fig-cap: "Density distributions show the spread of $Y$ values at each discrete $X$; black line connects the conditional means."

cef_df <- data.frame(
  X = rep(0:20, each = 100),
  Y = unlist(
    lapply(
      0:20,
      function(x) rnorm(100, mean = 6 + x * runif(1, 0.15, 0.25), sd = 0.5)
    )
  )
)

# Summarize data for the CEF line
cef_line <- cef_df |>
  group_by(X) |>
  summarise(mean_Y = mean(Y))

# Create plot
ggplot(cef_df, aes(x = X, y = Y)) +
  ggdist::stat_halfeye(
    adjust = .33, ## bandwidth
    width = .67,
    color = NA, ## remove slab interval
    fill = "#b16286", ## remove slab interval
    position = position_nudge(x = .15),
    alpha = 0.8
  ) +
  geom_jitter(
    size = 1,
    alpha = 0.3,
    width = .1,
    height = 0,
    color = "#458588"
  ) + # Scatter of data points
  geom_line(
    data = cef_line,
    aes(x = X, y = mean_Y),
    color = "black",
    linewidth = 1
  ) + # CEF line
  geom_point(
    data = cef_line,
    aes(x = X, y = mean_Y),
    color = "black",
    size = 1.5
  ) + # CEF line
  labs(
    x = "X",
    y = "Y"
  )

```

- The CEF is the _average line_ through the scatter of data points for each discrete $X$.

## Why Does CEF Matter?

<br><br>

- The CEF properties we just established are important because:

  1. [Decomposition]{.highlight}: Any outcome can be split into a systematic part (explained by covariates) and noise.

  2. [Optimality]{.highlight}: The CEF is the best predictor of $Y_i$ given $X_i$ in the MSE sense.

. . .

- [Key insight]{.note}: If we can estimate $\E[Y_i \given T_i]$ or $\E[Y_i \given T_i, X_i]$ well, we can estimate differences in conditional means—which under the right assumptions are causal effects.

. . .

- The question becomes: **Can regression help us estimate the CEF?**

# Regression Justification

## Regression?

<br><br>

- The $\E [Y_i \given X_i]$ quantity looks very familiar, we already used it in $\E [Y_i \given T_i]$ or $\E [Y_i \given T_i, X_i]$.

- We want to see if regression helps us with estimating these quantities. Especially when we want to estimate differences in means.

. . .

- [Note]{.note}: There is nothing _causal_ in $\E [Y_i \given T_i]$ or $\E [Y_i \given T_i, X_i]$, so we still need identification

  - We can for example rely on strong or conditional ignorability.

## Some Regression Coefficient Properties

- Before we move on to this, we need to recall important facts about regression coefficients

  :::incremental
  1. Population regression coefficients vector is given by (directly follows from $\E [X_i \varepsilon_i] = 0$)
  $$
  \beta = \E [X_i X_i^{\prime}]^{-1} \E [X_i Y_i]
  $$

  2. Regression coefficient in single covariate case is given by (population and sample analog)
  $$
  \beta = \frac{\cov(Y_i,X_i)}{\var (X_i)}, \quad \widehat{\beta} = \frac{\sum_{i = 1}^{n} (Y_i - \bar{Y})(X_i - \bar{X})}{\sum_{i = 1}^{n} (X_i - \bar{X})^2}
  $$

  3. Regression coefficient in multiple covariate case is given by
  $$
  \beta_{k} = \frac{\cov(\tilde{Y}_i,\tilde{X}_{ki})}{\var (\tilde{X}_{ki})},
  $$
  where $\tilde{X}_{ki}$ is the residual from regressing $X_k$ on $X_{-k}$
  :::

## Justification 1: Linearity

<br>

:::{.callout-important icon="false" title="Theorem: Linear CEF"}
If CEF $\E [Y_i \given X_i]$ is linear in $X_i$, then the population regression function $X_i^{\prime} \beta$ returns exactly $\E [Y_i \given X_i]$.
:::

. . .

- To see this property we can 

  - Use decomposition property of CEF to see $\E[ X_i (Y_i - \E [Y_i \given X_i]) ] = 0$

  - Substitute for $\E [Y_i \given X_i] = X_i^{\prime} b$ and solve

. . .

- How plausible is this _linearity_ assumption in practice?

. . .

- Always true in the simple case where $T_i$ is a binary treatment indicator: $\E [Y_i \given T_i] = \beta_0 + \beta_1 T_i$.

## Binary Case CEF

<br>

```{r}
#| label: cef_binary
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

cef_bin_df <- data.frame(
  X = rep(0:1, each = 100),
  Y = unlist(
    lapply(
      0:1,
      function(x) rnorm(100, mean = 6 + x * runif(1, 0.15, 0.25), sd = 0.5)
    )
  )
)

# Summarize data for the CEF line
cef_bin_line <- cef_bin_df |>
  group_by(X) |>
  summarise(mean_Y = mean(Y))

# Create plot
ggplot(cef_bin_df, aes(x = X, y = Y)) +
  geom_jitter(
    size = 1,
    alpha = 0.3,
    width = .05,
    height = 0,
    color = "#458588"
  ) +
  # same line
  stat_smooth(method = "lm", se = FALSE) +
  # same line
  geom_line(
    data = cef_bin_line,
    aes(x = X, y = mean_Y),
    color = "black",
    linewidth = 1
  ) +
  geom_point(
    data = cef_bin_line,
    aes(x = X, y = mean_Y),
    color = "black",
    size = 1.5
  ) +
  scale_x_continuous(breaks = c(0, 1)) +
  labs(
    x = "T",
    y = "Y"
  )

```

## Justification 2: Linear Approximation

- What if the CEF is not linear?

. . .

- Regression can still be used to approximate the CEF:

:::{.callout-important icon="false" title="Regression Prediction Property"}
The function $X_i' \beta$ provides the Minimal MSE linear approximation to $\E [Y_i \given X_i]$, that is:

$$
\beta = \argmin_b \E \left[ (\E [Y_i \given X_i] - X_i' b)^2 \right].
$$
:::

. . .

- [Intuition]{.note}: Even if CEF is not linear we can use regression to approximate it and make substantive conclusions

. . .

- To see this we can decompose the squared error function minimized by OLS

$$
\begin{align*}
(Y_i - X_i' b)^2 &= \left( (Y_i - \E [Y_i \given X_i]) + (\E [Y_i \given X_i] - X_i' b) \right)^2 \\
&= (Y_i - \E [Y_i \given X_i])^2 + (\E [Y_i \given X_i] - X_i' b)^2 \\
&\quad + 2 (Y_i - \E [Y_i \given X_i]) (\E [Y_i \given X_i] - X_i' b).
\end{align*}
$$

  - The first term doesn't involve $b$.
  - The last term has an expectation of zero due to the CEF-decomposition property.

## Approximation of Discrete Case CEF

<br>

```{r}
#| label: cef_approximation_example
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Create plot
ggplot(cef_df, aes(x = X, y = Y)) +
  geom_jitter(
    size = 1,
    alpha = 0.3,
    width = .1,
    height = 0,
    color = "#458588"
  ) + # Scatter of data points
  stat_smooth(method = "lm", se = FALSE, color = "#cc241d") +
  geom_line(
    data = cef_line,
    aes(x = X, y = mean_Y),
    color = "black",
    linewidth = 1
  ) + # CEF line
  geom_point(
    data = cef_line,
    aes(x = X, y = mean_Y),
    color = "black",
    size = 1.5
  ) + # CEF line
  labs(
    x = "X",
    y = "Y"
  )

```

## What Does This All Mean?

<br><br><br>

- In the case of CEF with respect to binary $X_i$ (think $T_i$), OLS provides an estimate of $\E [Y_i \given X_i]$ which is the same as difference in means.

. . .

- In the case of CEF linear in $X_i$, OLS provides estimate of $\E [Y_i \given X_i]$ which is (constant) increase in means of $Y_i$.

. . .

- In the case of CEF non-linear in $X_i$, OLS provides the best linear approximation of $\E [Y_i \given X_i]$

# Regression and Causality

## Back to Simple Binary Setup

<br>

- Suppose $\mathcal{T} = \{0, 1\}$

- Under SUTVA (no interference and consistency) POs are $Y_{i} (1)$ and $Y_{i} (0)$.

- A unit-level treatment effect is, $\tau_i = Y_{i} (1) - Y_{i} (0)$

  - $\E [\tau_i] = \E [Y_{i} (1) - Y_{i} (0)] = \tau_{ATE}$ is the average treatment effect (_ATE_).

- We observe $X_i$, $T_i$ and, $Y_i = T_i Y_{i} (1) + (1 - T_i )Y_{i} (0)$.

. . .

- In this simple case OLS estimator solves the least squares problem:
  
  $$
  (\widehat{\tau}, \widehat{\alpha}) = \argmin_{\tau, \alpha} \sum_{i=1}^n \left(Y_i - \alpha - \tau T_i\right)^2
  $$

- Coefficient $\tau$ is algebraically equivalent to the difference in means ($\tau_{DiM}$):
  
  $$
  \widehat{\tau} = \bar{Y}_1 - \bar{Y}_0 = \widehat{\tau}_{DiM}
  $$

## Regression Justification

- Key assumptions: **linearity** and **mean independence of errors**.  (why do we care about the latter?)

. . .

- Using the switching equation we can show that:

$$
\begin{align*}
Y_i &= T_i Y_i(1) + (1 - T_i) Y_i(0) \\
&= Y_i(0) + T_i ( Y_i(1) - Y_i(0) ) \quad\text{($\because$ distribute)}\\
&= Y_i(0) + \tau_i T_i \quad \text{($\because$ unit treatment definition)}\\
&= \E [Y_i(0)] + \tau T_i + ( Y_i(0) - \E [Y_i(0)] ) + T_i (\tau_i - \tau) \quad (\because \pm \E [Y_i(0)] + \tau T_i)\\
&= \E [Y_i(0)] + \tau T_i + (1 - T_i)( Y_i(0) - \E [Y_i(0)] ) + T_i (Y_i(1) - \E [Y_i(1)]) \quad\text{($\because$ distribute)}\\
&= \alpha + \tau T_i + \eta_i
\end{align*}
$$

. . .

- _Linear_ functional form fully justified by SUTVA assumption alone:
  
  - [Intercept]{.highlight}: $\alpha = \E [Y_i(0)]$ (average control outcome).
  - [Slope]{.highlight}: $\tau = \E [Y_i(1) - Y_i(0)]$ (average treatment effect).
  - [Error]{.highlight}: deviation of control PO + treatment effect heterogeneity. What is the second interpretation?

## Mean independent errors

- The error is given by

$$
\eta_i = (1 - T_i)( Y_i(0) - \E [Y_i(0)] ) + T_i (Y_i(1) - \E [Y_i(1)])
$$

- In the regression context we would like $\E [\eta_i \given T_i] = 0$?

. . .

$$
\begin{align*}
\E [\eta_i \given T_i] &= \E [(1 - T_i)( Y_i(0) - \E [Y_i(0)] ) + T_i (Y_i(1) - \E [Y_i(1)]) \given T_i] \\
&= (1 - T_i) (\E [Y_i(0) \given T_i] - \E [Y_i(0)]) + T_i (\E [Y_i(1) \given T_i] - \E [Y_i(1)])
\end{align*}
$$

- Does this look familiar? [This is selection with respect to $Y_i(0)$ and $Y_i(1)$.]{.fragment}

. . .

- When would this be equal to zero? [E.g. under random assignment (strong ignorability)]{.fragment}

<!-- $$\E [\eta_i \given T_i] = (1 - T_i) (\E [Y_i(0)] - \E [Y_i(0)]) + T_i (\E [Y_i(1)] - \E [Y_i(1)]) = 0$$ -->

. . .

- **Randomization + consistency allow linear model.**

  - Does not imply homoskedasticity or normal errors, though!

  - [Practical implication]{.note}: Use heteroskedasticity-robust (HC2) standard errors for inference, e.g. via `lm_robust()`.

# Regression with Covariates

## Why Control for Covariates?

<br>

- We just showed: under [strong ignorability]{.highlight} (random assignment), regression estimates causal effects.

- But what if treatment is **not** randomly assigned?

. . .

- [Key insight]{.note}: If we can identify variables $X_i$ that explain _why_ some units are treated and others are not, we may still recover causal effects.

. . .

- This is the [selection on observables]{.highlight} framework:

  - Treatment assignment depends on observable characteristics $X_i$
  - Once we account for $X_i$, treatment is "as good as random"
  - We can use regression to adjust for $X_i$ and estimate causal effects

. . .

- [Question]{.alert}: What assumptions do we need? And how do we implement this with regression?

## Conditional Ignorability

<br>

:::{.callout-important icon="false" title="Assumption: Conditional Ignorability (CIA)"}
$$
\{ Y_i(0), Y_i(1) \} \indep T_i \given X_i
$$

Given covariates $X_i$, treatment assignment is independent of potential outcomes.
:::

. . .

- [Interpretation]{.note}: Within groups defined by $X_i$, treatment is effectively randomized.

  - Units with the same $X_i$ who are treated vs. untreated are comparable.
  - Any remaining differences in outcomes reflect the causal effect of treatment.

. . .

- This is weaker than strong ignorability: we allow $T_i$ to depend on $X_i$, just not on $(Y_i(0), Y_i(1))$ after conditioning on $X_i$.

. . .

- [Critical requirement]{.alert}: We must observe and correctly measure _all_ confounders $X_i$.

## Modeling Potential Outcomes with Covariates

- To connect CIA to regression, we need to model how potential outcomes relate to covariates.

. . .

- Assume [constant treatment effects]{.highlight}: $\tau_i = \tau$ for all $i$.

- Potential outcomes follow: $f_i(t) = \alpha + \tau t + \eta_i$

  - $\alpha$: baseline expected outcome
  - $\tau$: constant causal effect of treatment
  - $\eta_i$: individual-specific deviation (captures everything else affecting $Y_i$)

. . .

- The error $\eta_i$ may depend on covariates. Decompose it as:

$$
\eta_i = X_i^{\prime} \gamma + \nu_i,
$$

  where $\gamma$ captures the linear relationship between $X_i$ and outcomes, and $\nu_i$ is the residual variation.

. . .

- [Assumption]{.highlight}: $\E [\eta_i \given X_i] = X_i^\prime \gamma$ (linearity in covariates) $\Rightarrow$ $\E [\nu_i \given X_i] = 0$.

## From Potential Outcomes to Regression

- Substituting the error decomposition into observed outcomes:

$$
Y_i = f_i(T_i) = \alpha + \tau T_i + X_i^{\prime} \gamma + \nu_i
$$

. . .

- This looks like a regression equation. But when does OLS identify $\tau$ causally?

. . .

- Under CIA, $f_i(t) \indep T_i \given X_i$, which implies:

$$
\nu_i \indep T_i \given X_i
$$

  - [Why?]{.note} Since $X_i$ is fixed, only $\nu_i$ varies in $\eta_i$. CIA on POs transfers to $\nu_i$.

. . .

- [Result]{.highlight}: The regression error $\nu_i$ is:

  1. Uncorrelated with $X_i$ (by construction of $\gamma$)
  2. Uncorrelated with $T_i$ conditional on $X_i$ (by CIA)

- Therefore, OLS on $Y_i = \alpha + \tau T_i + X_i^{\prime} \gamma + \nu_i$ yields consistent estimates.

## Causal Identification with Covariates

- Let's verify that $\tau$ captures the causal effect under our assumptions.

. . .

- By CIA, conditioning on $X_i$ removes selection bias:

$$
\E [f_i(t) \given T_i = t, X_i] = \E [f_i(t) \given X_i] = \alpha + \tau t + X_i^{\prime} \gamma
$$

. . .

- The causal effect of changing treatment from $(t-v)$ to $t$:

$$
\begin{align*}
\E [f_i(t) - f_i(t - v) \given X_i] &= (\alpha + \tau t + X_i^{\prime} \gamma) - (\alpha + \tau (t - v) + X_i^{\prime} \gamma) \\
&= \tau v
\end{align*}
$$

. . .

- [Key observation]{.note}: The $X_i^{\prime} \gamma$ terms cancel out!

  - Confounding enters the model additively and is differenced away.
  - The coefficient $\tau$ represents the causal effect of a unit change in $T_i$.

## Regression with Covariates: Takeaways

<br>

- [What we assumed]{.highlight}:

  1. **Conditional ignorability**: $(Y_i(0), Y_i(1)) \indep T_i \given X_i$
  2. **Constant effects**: $\tau_i = \tau$ for all units
  3. **Linearity**: $\E [\eta_i \given X_i] = X_i^\prime \gamma$

. . .

- [What we achieved]{.highlight}:

  - OLS regression of $Y_i$ on $T_i$ and $X_i$ consistently estimates the causal effect $\tau$
  - Covariates "absorb" confounding, leaving only causal variation in $T_i$

. . .

- [Critical question]{.alert}: What happens if we **omit** important confounders from $X_i$?

  - If CIA fails because we missed a confounder, our estimate will be biased.
  - This leads us to [omitted variable bias]{.highlight}...

# Omitted Variable Bias

## Omitted Variable Bias (OVB) {#ovb}

- Now suppose we _erroneously_ omit $X_i$, and just regress $Y_i$ on $T_i$ via OLS.  

. . .

- To see [omitted variable bias]{.highlight} we look at what the coefficient on $T_i$ estimates, $\frac{\cov(Y_i, T_i)}{\var(T_i)}$ assuming that the true model should include $X_i$:

$$
\begin{align*}
\cov(Y_i, T_i) &= \cov(\alpha + \tau T_i + X_i' \gamma + \nu_i,\, T_i) \\
&= \tau \cov(T_i, T_i) + \cov(X_{1i} \gamma_1 + \ldots + X_{Ki} \gamma_K, T_i) \\
&= \tau \var (T_i) + \gamma_1 \cov(X_{1i}, T_i) + \ldots + \gamma_K \cov(X_{Ki}, T_i)
\end{align*}
$$  

. . .

$$
\implies \frac{\cov(Y_i, T_i)}{\var (T_i)} = \tau + \underbrace{\gamma^{\prime} \delta}_{\text{OVB}}
$$

where $\delta$ are coefficients from regressions of $X_1, \ldots, X_K$ on $T_i$.

. . .

- By the [Frisch–Waugh–Lovell theorem](#fwl-theorem), if we include some of $X_i$ we will get $\frac{\cov(\tilde{Y}_i, \tilde{T}_i)}{\var (\tilde{T}_i)} = \tau + \tilde{\gamma}^{\prime}\tilde{\delta}$, where $\tilde{\cdot}$ means residualized with respect to included terms from $X_i$.

## Omitted Variable Bias

<br>

- [OVB]{.highlight} = $\gamma^\prime \delta$, where

  - $\gamma$ is the vector of effects of [confounders]{.highlight} on the outcome.
  - $\delta$ is the vector of associations between [confounders and treatment]{.highlight} — i.e., the degree of confounder-induced imbalance in treatment assignment.

. . .

- Same holds when we consider the case where we include _some_ controls: 
  
  $$
  \text{OVB} = \tilde{\gamma}' \tilde{\delta}.
  $$
  
  Everything is just defined in terms of variables that have been residualized with respect to the included controls.

- OVB = [confounder impact $\times$ imbalance]{.highlight} [@cinelli2020making].

## Omitted Variable Bias


- Let's practice applying the OVB formula:

  OVB = $(X_{ki}, Y_i)$ relationships $\times$ $(X_{ki}, T_i)$ relationships

```{dot}
//| fig-width: 4
//| fig-height: 2
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="LR"
  newrank=true;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=3,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    shape=plaintext,
    fontsize=24,
    penwidth=2
  ]
  Y;
  T->Y;

  subgraph U {
    node [
      shape=plaintext
      fontcolor="#cc241d"
      fontsize=24
      penwidth=2
    ]
		edge [color = "#cc241d"]
  X->Y;X->T;
}
}
```

. . .

1.	Effect of democratic institutions on growth, estimated via regression of growth on democratic institutions.

2.	Effect of exposure to negative advertisements on turnout, estimated via regression of turnout on the number of ads seen.

- [Question]{.note}: What is a possible omitted variable? How will this bias the estimate?

## OVB: Simulate DAG Relationship

```{r}
#| echo: true
#| eval: true
#| output: asis

set.seed(20250127) # set seed

n <- 1000 # sample size
tau <- 0.5 # ATE
gamma <- 0.3 # effect of confounder on outcome
delta <- 0.3 # effect of confounder on treatment

# confounder
confounder <- rnorm(n, mean = 50, sd = 10)

# democratic institutions (correlated with confounder)
democracy_score <- delta * confounder + rnorm(n, mean = 0, sd = 5)

# economic growth (influenced by both investment and democratic institutions)
growth <- tau *
  democracy_score +
  gamma * confounder +
  rnorm(n, mean = 0, sd = 5)

# true regression including the confounder
model_unbiased <- lm(growth ~ democracy_score + confounder)
cat("Unbiased model error:", unname(model_unbiased$coefficients[2]) - tau, "\n")

# regression ignoring the confounder
model_biased <- lm(growth ~ democracy_score)
cat("Biased model error:", unname(model_biased$coefficients[2]) - tau, "\n")
```

## OVB: High $\gamma$, High $\delta$

```{r}
#| label: ovb_hg_hd
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  pbapply::pbreplicate(
    1000,
    get_bias(delta = 0.5, gamma = 0.5),
    cl = "future"
  ) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(
    cols = c(Biased, Unbiased),
    names_to = "Type",
    values_to = "Value"
  )

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed",
    linewidth = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  )
```

## OVB: High $\gamma$, Low $\delta$

```{r}
#| label: ovb_hg_ld
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  pbapply::pbreplicate(
    1000,
    get_bias(delta = 0.1, gamma = 0.5),
    cl = "future"
  ) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(
    cols = c(Biased, Unbiased),
    names_to = "Type",
    values_to = "Value"
  )

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed",
    linewidth = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  )
```

## OVB: Low $\gamma$, High $\delta$

```{r}
#| label: ovb_lg_hd
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  pbapply::pbreplicate(
    1000,
    get_bias(delta = 0.5, gamma = 0.1),
    cl = "future"
  ) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(
    cols = c(Biased, Unbiased),
    names_to = "Type",
    values_to = "Value"
  )

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed",
    linewidth = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  )
```

## OVB: Low $\gamma$, Low $\delta$

```{r}
#| label: ovb_lg_ld
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

# Convert sims matrix to long format dataframe
sims_df <-
  pbapply::pbreplicate(
    1000,
    get_bias(delta = 0.1, gamma = 0.1),
    cl = "future"
  ) |>
  t() |>
  as.data.frame() |>
  rename(Biased = biased, Unbiased = unbiased) |>
  pivot_longer(
    cols = c(Biased, Unbiased),
    names_to = "Type",
    values_to = "Value"
  )

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Type)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#458588", "#cc241d"), name = "Model") +
  geom_vline(
    data = summarize(group_by(sims_df, Type), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Type),
    linetype = "dashed",
    linewidth = 1
  ) +
  scale_color_manual(values = c("#458588", "#cc241d"), name = "Model") +
  scale_x_continuous(limits = c(-0.15, 0.8)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  )
```


## Be Careful!

<br><br><br>

- **Omitted variables** is a misleading term because it could suggest that you want to include *any* variable that is correlated with treatment and outcomes.

- But remember [bad controls]{.highlight} exist, e.g.

   - Common descendants of treatment and outcome (colliders)
   - Block causal path by controlling for post-treatment variables

## Two Ways to Adjust for Covariates

<br>

- The discussion of OVB suggests that we can use regression to adjust for variables ($X_i$) to estimate the treatment effect ($\tau$) in two ways.

  1. [_Long_ regression]{.highlight}: Include covariates $X_i$ directly in the regression model.

  2. [Residualized regression]{.highlight}:
     a. Purge variation in $Y_i$ due to $X_i$ $\rightarrow$ Regress $Y_i$ on $X_i$ and calculate residual outcomes: $\tilde{Y}_i = Y_i - \widehat{Y}_i$.
     b. Purge variation in $T_i$ due to $X_i$ $\rightarrow$ Regress $T_i$ on $X_i$ and calculate residual treatments: $\tilde{T}_i = T_i - \widehat{T}_i$.
     c. Regress $\tilde{Y}_i$ on $\tilde{T}_i$.

- [Result]{.note}: Coefficient on $T_{i}$ in _long_ regression and on $\tilde{T}_i$ in residualized regression are identical.

# Back-Door Criterion

## Identification Analysis with Causal Graphs

- An alternative, perhaps more intuitive, way to think about confounding is in terms of DAGs.
- Suppose we want to estimate the _ATE_ of $T$ on $Y$; which covariates do we need to measure?

. . .

- Pearl develops criteria, which can be directly read off the graph alone.
- Before studying the criteria, we need to define some new concepts.

. . .

:::{.columns}
::: {.column width="70%"}

- [Nodes]{.highlight}: $T$, $Y$, $Z_1$, $Z_2$, and $Z_3$.

- [Paths]{.highlight}: $T \to Y$, $T \leftarrow Z_3 \to Y$, $T \leftarrow Z_1 \to Z_3 \leftarrow Z_2 \to Y$, etc.

- $Z_1$ is a [parent]{.highlight} of $T$ and $Z_3$.

- $T$ and $Z_3$ are [children]{.highlight} of $Z_1$.

- $Z_1$ is an [ancestor]{.highlight} of $Y$.

- $Y$ is a [descendant]{.highlight} of $Z_1$.

:::

::: {.column width="30%"}
```{dot}
//| fig-width: 3
//| fig-height: 3
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    // Main nodes
    T;
    Y;
    
    // Confounders
    Z1 [label=<Z<sub>1</sub>>];
    Z2 [label=<Z<sub>2</sub>>];
    Z3 [label=<Z<sub>3</sub>>];

    // Causal path
    T -> Y [color="#458588", penwidth=3];

    // Back-door paths (confounding)
    Z1 -> T [color="#cc241d"];
    Z1 -> Z3 [color="#cc241d"];
    Z2 -> Y [color="#cc241d"];
    Z2 -> Z3 [color="#cc241d"];
    Z3 -> T [color="#cc241d"];
    Z3 -> Y [color="#cc241d"];

    {{ rank=min; Z1;Z2};}
    {{ rank=same; Z3};}
    {{ rank=max; T;Y};}
}
```
:::
:::

## Back-Door vs. Causal Paths

:::{.columns}
::: {.column width="60%"}

::: {.callout-important icon="false" title="Definition: Types of Paths"}

A [causal (front-door) path]{.highlight} from $T$ to $Y$ is a path where every arrow points **away from** $T$ toward $Y$: $T \to \cdots \to Y$

A [back-door path]{.highlight} from $T$ to $Y$ is any path that starts with an arrow **into** $T$: $T \leftarrow \cdots$

:::

::: {.fragment fragment-index=1}
- [Causal paths]{.highlight} transmit the effect of $T$ on $Y$ — we want to keep these open!

- [Back-door paths]{.highlight} create spurious associations (confounding) — we want to block these.
:::

:::

::: {.column width="40%"}
```{dot}
//| fig-width: 3.5
//| fig-height: 3.5
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    T;
    Y;
    Z;
    W;

    // Causal path
    T -> W [color="#458588"];
    W -> Y [color="#458588"];

    // Back-door path
    Z -> T [color="#cc241d"];
    Z -> Y [color="#cc241d"];

    {{ rank=min; Z};}
    {{ rank=same; T;W};}
    {{ rank=max; Y};}
}
```
:::
:::

::: {.fragment fragment-index=2}
- [Example]{.note}: $T \to W \to Y$ is a [causal path]{style="color:#458588"}. $T \leftarrow Z \to Y$ is a [back-door path]{style="color:#cc241d"}.

- [Intuition]{.note}: Back-door paths are "alternative explanations" for why $T$ and $Y$ might be correlated, even if $T$ has no causal effect on $Y$.
:::

## Colliders: A Key Concept

<br>

:::{.columns}
::: {.column width="65%"}

- A [collider]{.highlight} on a path is a node where two arrows "collide" (point into it): $\to C \leftarrow$

::: {.fragment fragment-index=1}
- [Key insight]{.note}: Colliders have special properties:

  - A path through a collider is [naturally blocked]{.highlight} — no information flows through it.
  - [Conditioning on a collider **opens** the path!]{.alert} This can create spurious associations.
:::

::: {.fragment fragment-index=2}
- [Example]{.note}: $Z_3$ is a collider on the path $Z_1 \to Z_3 \leftarrow Z_2$.

  - Without conditioning: $Z_1 \indep Z_2$ (path blocked).
  - Conditioning on $Z_3$: $Z_1 \notindep Z_2 \given Z_3$ (path opened!).
:::

:::

::: {.column width="35%"}
```{dot}
//| fig-width: 3
//| fig-height: 2.5
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2.5]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    Z1 [label=<Z<SUB>1</SUB>>];
    Z2 [label=<Z<SUB>2</SUB>>];
    // Collider highlighted
    Z3 [label=<Z<SUB>3</SUB>>, fontcolor="#cc241d"];

    // Arrows pointing INTO collider
    Z1 -> Z3 [color="#cc241d"];
    Z2 -> Z3 [color="#cc241d"];
    {{ rank=min; Z1;Z2};}
    {{ rank=max; Z3};}
}
```
:::
:::

## Blocking Paths

:::{.columns}
::: {.column width="60%"}
::: {.callout-important icon="false" title="Definition: Blocked Paths"}

A path $p$ is **blocked** by a set of nodes $X$ if:

1. $p$ contains a [non-collider]{.highlight} that is in $X$ (conditioning blocks flow), **OR**
2. $p$ contains a [collider]{.highlight} where neither the collider nor its descendants are in $X$ (naturally blocked).

:::

::: {.fragment fragment-index=1}
- [Intuition]{.note}: Conditioning on non-colliders blocks information flow; conditioning on colliders opens it.
:::

:::

::: {.column width="40%"}
```{dot}
//| fig-width: 3
//| fig-height: 3
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    // Main nodes
    T;
    Y;
    // Collider (special)
    Z3 [label=<Z<SUB>3</SUB>>];
    // Other nodes
    Z1 [label=<Z<SUB>1</SUB>>];
    Z2 [label=<Z<SUB>2</SUB>>];
    W1 [label=<W<SUB>1</SUB>>];
    W2 [label=<W<SUB>2</SUB>>];
    W3 [label=<W<SUB>3</SUB>>];

    // Back-door paths
    Z1 -> W1 [color="#cc241d"];
    W1 -> T [color="#cc241d"];
    Z1 -> Z3 [color="#cc241d"];
    Z2 -> W2 [color="#cc241d"];
    W2 -> Y [color="#cc241d"];
    Z2 -> Z3 [color="#cc241d"];
    Z3 -> T [color="#cc241d"];
    Z3 -> Y [color="#cc241d"];

    // Causal path
    T -> W3 [color="#458588", penwidth=3];
    W3 -> Y [color="#458588", penwidth=3];

    {{ rank=min; Z1;Z2};}
    {{ rank=same; W1;Z3;W2};}
    {{ rank=max; T;W3;Y};}
}
```
:::
:::

::: {.fragment fragment-index=2}
- $T \leftarrow W_1 \leftarrow Z_1 \to Z_3 \to Y$: blocked by $\{W_1\}$ or $\{Z_1\}$ (non-colliders).
- $T \leftarrow W_1 \leftarrow Z_1 \to Z_3 \leftarrow Z_2 \to W_2 \to Y$: blocked by $\{\emptyset\}$ ($Z_3$ is a collider).
:::

## $d$-Separation

<br><br><br>

::: {.callout-important icon="false" title="Definition: $d$-separation"}
A set $X$ [$d$-separates]{.highlight} $T$ and $Y$ if $X$ blocks **all** paths between $T$ and $Y$.

If $X$ $d$-separates $T$ and $Y$, then $Y \indep T \given X$.
:::

- [Intuition]{.note}: $d$-separation is a graphical criterion for conditional independence — if all paths are blocked, the variables are independent given $X$.

## The Back-Door Criterion

::: {.callout-important icon="false" title="Theorem: The Back-Door Criterion"}
A set $X$ satisfies the back-door criterion relative to $(T, Y)$ if:

  1. $X$ [blocks ($d$-separates) all back-door paths]{.highlight} from $T$ to $Y$, **and**
  2. No element of $X$ is a [descendant of $T$]{.highlight}.
:::

::: {.fragment fragment-index=1}
- [Why condition 1?]{.note} Blocking back-door paths eliminates confounding — the spurious association between $T$ and $Y$.

- [Why condition 2?]{.note} Descendants of $T$ are post-treatment variables. Conditioning on them could _block_ part of the causal effect (if they’re mediators), and/or induce collider/selection bias (if they’re affected by $T$ and share causes with $Y$, etc.).
:::

::: {.fragment fragment-index=2}
- [Result]{.highlight}: If $X$ satisfies the back-door criterion, then it implies conditional ignorability ($Y_i(t) \indep T_i \given X_i$):

$$
\begin{align*}
\E[Y_i(t)] &= \E_X[\E[Y_i \given T_i = t, X_i]] \implies \\
\implies\ \tau_{ATE} &= \E[Y_i(1)] - \E[Y_i(0)] = \E_X[\E[Y_i \given T_i = 1, X_i] - \E[Y_i \given T_i = 0, X_i]]
\end{align*}
$$
:::

## Back-Door Criterion: Example

<br>

:::{.columns}
::: {.column width="40%"}
```{dot}
//| fig-width: 4
//| fig-height: 3
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir="TB"
  edge [
		arrowsize=0.5, labeldistance=3, penwidth=2]
  node [
    shape=plaintext,fontsize=24,penwidth=2]
    // Main nodes
    T;
    Y;
    // Collider (special - conditioning opens paths!)
    Z3 [label=<Z<SUB>3</SUB>>];
    // Other confounders
    Z1 [label=<Z<SUB>1</SUB>>];
    Z2 [label=<Z<SUB>2</SUB>>];
    W1 [label=<W<SUB>1</SUB>>];
    W2 [label=<W<SUB>2</SUB>>];
    // Post-treatment (descendant of T)
    W3 [label=<W<SUB>3</SUB>>];

    // Back-door paths (red)
    Z1 -> W1 [color="#cc241d"];
    W1 -> T [color="#cc241d"];
    Z1 -> Z3 [color="#cc241d"];
    Z2 -> W2 [color="#cc241d"];
    W2 -> Y [color="#cc241d"];
    Z2 -> Z3 [color="#cc241d"];
    Z3 -> T [color="#cc241d"];
    Z3 -> Y [color="#cc241d"];

    // Causal path (blue, thicker)
    T -> W3 [color="#458588", penwidth=3];
    W3 -> Y [color="#458588", penwidth=3];

    {{ rank=min; Z1;Z2};}
    {{ rank=same; W1;Z3;W2};}
    {{ rank=max; T;W3;Y};}
}
```
:::

::: {.column width="60%"}

- [Back-door paths]{.highlight} from $T$ to $Y$:
  1. $T \leftarrow Z_3 \to Y$
  2. $T \leftarrow W_1 \leftarrow Z_1 \to Z_3 \to Y$
  3. $T \leftarrow W_1 \leftarrow Z_1 \to Z_3 \leftarrow Z_2 \to W_2 \to Y$

::: {.fragment fragment-index=1}
- [Example]{.note}: Which sets satisfy the back-door criterion?
  - $X = \{W_1, W_2\}$? [**No** — doesn't block path 1.]{.fragment fragment-index=2}
  - $X = \{Z_3\}$? [**No** — opens path 3 (collider!).]{.fragment fragment-index=3}
  - $X = \{Z_1, Z_3\}$? [**Yes!** Blocks all back-door paths.]{.fragment fragment-index=4}
  - $X = \{W_3\}$? [**No** — $W_3$ is a descendant of $T$.]{.fragment fragment-index=5}
:::
:::
:::

# _The Good, The Bad, The Ugly..._ Controls

## Choosing Controls: A Taxonomy

<br><br>

- Follow @cinelli2024crash which provides a systematic framework for thinking about control variables.

- [Key insight]{.note}: Not all variables that are correlated with treatment and outcome should be controlled for!

- We will classify controls as:
  1. [Good controls]{.highlight}: Block back-door paths without introducing bias
  2. [Neutral controls]{.highlight}: Neither help nor hurt identification (but may affect precision)
  3. [Bad controls]{.highlight}: Introduce bias through collider conditioning or post-treatment adjustment

## Good Controls 1

:::{.columns}
::: {.column  width="50%"}

<br>

- [confounder]{.highlight} is a common cause of main explanatory variable, $X$, and outcome of interest, $Y$.

:::fragment
- In model (a) $Z$ is a common cause of $X$ and $Y$. Controlling for $Z$ blocks the back-door path.
:::

:::fragment
- In models (b) and (c) $Z$ is not a common cause, but controlling for $Z$ blocks the back-door path due to unobserved confounder $U$.
:::

:::
::: {.column  width="50%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]
  
  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_c [style=dashed, label="U"];
    
    U_c -> X_c -> Y_c;
    U_c -> Z_c -> Y_c;
    { rank=min; Z_c}
    { rank=max; X_c;Y_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_b [style=dashed, label="U"];
    
    U_b -> Z_b -> X_b -> Y_b;
    U_b -> Y_b;
    { rank=min; Z_b}
    { rank=max; X_b;Y_b}
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;

    Z [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    Z -> X -> Y;
    Z -> Y;
    { rank=min; Z}
    { rank=max; X;Y}
  }

}
```
:::
:::

## Good Controls 2

:::{.columns}
::: {.column  width="55%"}

<br><br>

- [Intuition]{.note}: Common causes of $X$ and any mediator $M$ (between $X$ and $Y$) also
confound the effect of $X$ on $Y$. 

:::fragment
- Models (a)-(c) are analogous to the models without mediator -- controlling for $Z$ blocks the back-door path from $X$ to $Y$ (through $M$) and produces an unbiased estimate of the _ATE_.
:::

:::
::: {.column  width="45%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]
  
  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    M_c [label="M"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_c [style=dashed, label="U"];
    
    U_c -> X_c -> M_c -> Y_c;
    U_c -> Z_c -> M_c;
    { rank=min; Z_c}
    { rank=same; X_c;M_c}
    { rank=max; Y_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    M_b [label="M"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    U_b [style=dashed, label="U"];
    
    U_b -> Z_b -> X_b -> M_b -> Y_b;
    U_b -> M_b;
    { rank=min; Z_b}
    { rank=same; X_b;M_b}
    { rank=max; Y_b}
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;

    Z [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    Z -> X -> M -> Y;
    Z -> M;
    { rank=min; Z}
    { rank=same; X;M}
    { rank=max; Y}
  }

}
```

:::
:::

## Neutral (?) Controls

:::{.columns}
::: {.column  width="55%"}

<br><br>

- [Intuition]{.note}: Ancestors of only $X$, only $Y$, or only $M$ (mediator) do not introduce bias. Controling for these factors will reduce variation in respective variable that is _not_ related to the variation in other variable.

:::fragment
- In model (a) reduction in variation is good! $\rightarrow$ higher precision

- In model (b) reduction in variation is bad! $\rightarrow$ lower precision

- In model (c) reduction in variation is good again! $\rightarrow$ higher precision
:::

:::
::: {.column  width="45%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]

  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    M_c [label="M"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    X_c -> M_c -> Y_c;
    Z_c -> M_c;
    { rank=min; X_c}
    { rank=same; Z_c;M_c}
    { rank=max; Y_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    Z_b -> X_b -> Y_b;
    {rank=same; Z_b;X_b};
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;
    Z [shape=circle, style=filled, fillcolor=red];
    X;
    Y;
    
    Z -> Y;
    X -> Y;
    {rank=same; Z;Y};
  }
}
```

:::
:::

## Bad Controls: Selection Bias

:::{.columns}
::: {.column  width="55%"}

<br><br>

- [Intuition]{.note}: We do not want to control for colliders or their descendants. This induces selection bias

:::fragment
- In models (a) and (b) controlling for $Z$ unblocks back-door paths and induces relationship between $X$ and $Y$.

- In models (c) and (d) controlling for $Z$ will unblock the back-door path $X \rightarrow Z \leftarrow U \rightarrow Y$.
:::

:::
::: {.column  width="45%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]

  // Diagram (d)
  subgraph cluster_d {
    label="(d)";
    peripheries=0;
    X_d [label="X"];
    Y_d [label="Y"];
    Z_d [shape=circle, style=filled, fillcolor=red, label="Z"];
    U1_d [style=dashed, label="U1"];
    U2_d [style=dashed, label="U2"];
    
    U1_d -> Z_d;
    U2_d -> Z_d;
    Z_d -> Y_d;
    U1_d -> X_d;
    U2_d -> Y_d;
    X_d -> Y_d;
    { rank=min; U1_d;X_d}
    { rank=same; Z_d;}
    { rank=max; U2_d;Y_d;}
  }

  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    U1_c [style=dashed, label="U1"];
    U2_c [style=dashed, label="U2"];
    
    U1_c -> Z_c;
    U2_c -> Z_c;
    U1_c -> X_c;
    U2_c -> Y_c;
    X_c -> Y_c;
    { rank=min; U1_c;X_c}
    { rank=same; Z_c;}
    { rank=max; U2_c;Y_c;}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    Y_b [label="Y"];
    U_b [style=dashed, label="U"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    X_b -> Z_b ;
    U_b -> Z_b;
    U_b -> Y_b;
    X_b -> Y_b;
    { rank=min; X_b}
    { rank=same; Z_b;U_b}
    { rank=max; Y_b;}
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;
    Z [shape=circle, style=filled, fillcolor=red];
    X;
    Y;
    
    X -> Y -> Z;
    X -> Z;
    { rank=min; X}
    { rank=same; Z}
    { rank=max; Y}
  }
}
```
:::
:::

## Bad Controls: Selection Bias

![](../_images/selection_bias.png){fig-align="center"}

## Bad Controls: Post-Treatment Bias

:::{.columns}
::: {.column  width="55%"}

<br><br>

- [Intuition]{.note}: We do not want to block the _channels_ through which the effect goes (unless we are interested in $CATE$). This induces post-treatment bias

:::fragment

- In models (a) and (b) controlling for $Z$ blocks the causal path.

- In model (c) controlling for $Z$ blocks part of the causal path.

- In model (d) controlling for $Z$ will **not** block the causal path or induce any bias.

:::

:::
::: {.column  width="45%"}

```{dot}
//| fig-width: 4
//| fig-height: 6
//| fig-align: center
digraph G {
  bgcolor="transparent"
  rankdir=LR;
  edge [
		arrowsize=0.5,
		// fontname="Helvetica,Arial,sans-serif"
		labeldistance=5,
		// labelfontcolor="#00000080"
		penwidth=2
		// style=dotted // dotted style symbolizes data transfer
	]
  node [
    fontsize=24,
    penwidth=2
  ]

  // Diagram (d)
  subgraph cluster_d {
    label="(d)";
    peripheries=0;
    X_d [label="X"];
    Y_d [label="Y"];
    Z_d [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    X_d -> Z_d;
    X_d -> Y_d;
    { rank=min; X_d}
    { rank=same; Z_d}
    { rank=max; Y_d}
  }

  // Diagram (c)
  subgraph cluster_c {
    label="(c)";
    peripheries=0;
    X_c [label="X"];
    Y_c [label="Y"];
    Z_c [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    X_c -> Z_c -> Y_c;
    X_c -> Y_c;
    { rank=min; X_c}
    { rank=same; Z_c}
    { rank=max; Y_c}
  }

  // Diagram (b)
  subgraph cluster_b {
    label="(b)";
    peripheries=0;
    X_b [label="X"];
    M_b [label="M"];
    Y_b [label="Y"];
    Z_b [shape=circle, style=filled, fillcolor=red, label="Z"];
    
    X_b -> M_b -> Y_b;
    M_b -> Z_b
    {rank=same; M_b;Z_b};
  }

  // Diagram (a)
  subgraph cluster_a {
    label="(a)";
    peripheries=0;
    Z [shape=circle, style=filled, fillcolor=red];
    X;
    Y;
    
    X -> Z -> Y;
  }
}
```

:::
:::

## Bad Controls: Post-Treatment Bias

<br>

- To see the intuition behind [post-treatment bias]{.highlight} consider the following example

- Suppose $X = 0, 1$ randomly assigned, and then

  $$
  \begin{align*}
  Z &= X + \varepsilon_Z, \\
  Y &= \beta X + \gamma Z + \varepsilon_Y,
  \end{align*}
  $$

  where $\varepsilon_Z$ and $\varepsilon_Y$ are independent standard normal draws.

. . .

- Substituting in $Y$:  
  
  $$
  Y = (\beta + \gamma)X + \gamma \varepsilon_Z + \varepsilon_Y
  $$

. . .

- Effect of $X$ on $Y$ is $\beta + \gamma$.

- Controlling for $Z$, we would estimate an effect of $\beta$.

- The bias, $-\gamma$, is the portion of the effect that has been "stolen away" by conditioning on $Z$.

## Controls Conclusion

<br><br>

- Be mindful of what controls you include in your analysis (even if it is an experiment).

- Draw a DAG with controls you plan to include and see whether
  
  - You need them to _block_ any back-door paths.
  - They might be colliders or introduce post-treatment bias.
  - Do not use "kitchen sink" approach!

- Be also mindful of the sizes of the effects of potential confounders. If the effect on main independent and dependent variable can be proven to be limited, the OVB is small!

# Regression with Heterogeneous Treatments

## What If Treatment Effects Vary?

<br><br>

- Thus far we assumed [constant effects]{.highlight} ($\tau_i = \tau$) and [linearity]{.highlight} ($\E [\eta_i \given X_i] = X'_i \gamma$).

- These are strong assumptions! What happens if treatment effects vary across units?

. . .

- Setup with [heterogeneous effects]{.highlight}:

  - Binary treatment: $T_i \in \{0,1\}$
  - Potential outcomes: $Y_{i}(0)$, $Y_{i}(1)$
  - Unit-level treatment effect: $\tau_i = Y_{i}(1) - Y_{i}(0)$ (varies across $i$!)
  - Discrete covariate: $X_i \in \{x_1, x_2, \ldots, x_L\}$

. . .

- Maintain [conditional ignorability]{.highlight} (CIA): $\{ Y_{i}(0), Y_{i}(1) \} \indep T_i \given X_i$

- [Goal]{.note}: Estimate $\tau_{ATE} = \E[\tau_i]$ using regression.

## The Target: Average Treatment Effect

<br><br>

- Under CIA, the ATE can be written as a weighted average of [conditional ATEs]{.highlight}:

  $$
  \begin{align*}
  \tau_{ATE} &= \E[\tau_i] = \E_{X} [\E [Y_i(1) - Y_i(0) \given X_i]] \\
  &= \E_{X} [\underbrace{\E [Y_i(1) \given X_i] - \E [Y_i(0) \given X_i]}_{\tau_x}] \\
  &= \sum_{x} \tau_x \Pr(X_i = x),
  \end{align*}
  $$

  where $\tau_x \equiv \E [Y_i(1) - Y_i(0) \given X_i = x]$ is the CATE for stratum $x$.

. . .

- [Key insight]{.note}: The ATE averages stratum-specific effects $\tau_x$ by their population shares $\Pr(X_i = x)$.

## Estimating with Saturated Regression

<br><br>

- To flexibly control for $X_i$, use a [saturated regression]{.highlight} (fixed effects for each $X_i$ value):

  $$
  Y_i = \alpha_1 \mathbb{1}[X_i = x_1] + \cdots + \alpha_L \mathbb{1}[X_i = x_L] + \tau T_i + \varepsilon_i,
  $$

  where $\mathbb{1}[\cdot]$ is the indicator function. (One $\alpha$ omitted if including intercept.)

. . .

- [Why saturated?]{.note}

  - Makes no assumption about functional form of $\E[Y_i \given X_i]$
  - Each stratum $x$ gets its own intercept $\alpha_x$
  - This is the most flexible linear specification for discrete $X_i$

. . .

- [Question]{.alert}: Does $\widehat{\tau}$ from this regression estimate $\tau_{ATE}$?

## Regression Anatomy

- [Regression anatomy]{.highlight} is: $\widehat{\tau} = \frac{\cov(\tilde{Y}_i, \tilde{T}_i)}{\var(\tilde{T}_i)} = \frac{\cov(Y_i, \tilde{T}_i)}{\var(\tilde{T}_i)}$, where $\tilde{T}_i$ is residuals from regression of $T_i$ on other regressors.

- Let's see if it actually works

. . .

```{r}
#| label: regression_anatomy
#| echo: true
#| code-line-numbers: "1-5|7-8|10-12|14-18"
#| output-location: column-fragment

# simulate data
n <- 1000
X <- rnorm(n)
D <- 0.5 * X + rnorm(n) # do not use T!!!
Y <- 2 * D + 1 * X + rnorm(n)

# standard regression
standard <- coef(lm(Y ~ D + X))["D"]

# make Y tilde and D tilde
tilde_Y <- lm(Y ~ X)$residuals
tilde_D <- lm(D ~ X)$residuals

# regression anatomy
anatomy <- coef(lm(tilde_Y ~ tilde_D))["tilde_D"]

# simplified regression anatomy
anatomy_simp <- coef(lm(Y ~ tilde_D))["tilde_D"]

data.frame(
  Method = c("Standard", "Regression Anatomy", 
  "Regression Anatomy (Simplified)"),
  Coefficient = c(standard, anatomy, anatomy_simp)
) |>
  knitr::kable(digits = 3)
```

## Deriving What OLS Estimates

- Define the [residualized treatment]{.highlight}: $\tilde{T}_i \equiv T_i - \E[T_i \given X_i]$

- [Key property]{.note}: $\E[\tilde{T}_i] = 0$ (residuals have mean zero)

. . .

- Start from regression anatomy and simplify the covariance:

  $$
  \begin{align*}
  \widehat{\tau} &= \frac{\cov(Y_i, \tilde{T}_i)}{\var(\tilde{T}_i)} = \frac{\E[Y_i \tilde{T}_i] - \E[Y_i]\textcolor{#d65d0e}{\E[\tilde{T}_i]}}{\E[\tilde{T}_i^2]} \\
  &= \frac{\E[Y_i \tilde{T}_i]}{\E[\tilde{T}_i^2]}  \quad \text{($\because$ $\E[\tilde{T}_i] = 0$)}
  \end{align*}
  $$

. . .

- Apply [law of iterated expectations]{.highlight} to the numerator:

  $$
  \E[Y_i \tilde{T}_i] = \E\big[\E[Y_i \given T_i, X_i]  \tilde{T}_i\big]
  $$

  This works because $\tilde{T}_i$ is a function only of $T_i$ and $X_i$ and is constant when $T_i$ and $X_i$ are fixed.

. . .

- So we have: $\widehat{\tau} = \frac{\E\big[\E[Y_i \given T_i, X_i]  \tilde{T}_i\big]}{\E[\tilde{T}_i^2]}$

## Expanding the Conditional Expectation

<br><br><br>

- Expand $\E [Y_i \given T_i, X_i]$ using the [switching equation]{.highlight}: $Y_i = T_i Y_i(1) + (1-T_i) Y_i(0)$

  $$
  \begin{align*}
  \E [Y_i \given T_i, X_i] &= T_i \E [Y_{i}(1) \given T_i, X_i] + (1-T_i) \E [Y_{i}(0) \given T_i, X_i] \\
  &= T_i \E [Y_{i}(1) \given X_i] + (1-T_i) \E [Y_{i}(0) \given X_i] \quad \text{($\because$ CIA)}\\
  &= T_i \big(\E [Y_{i}(1) \given X_i] - \E [Y_{i}(0) \given X_i]\big) + \E [Y_{i}(0) \given X_i] \quad \text{($\because$ rearrange)}\\
  &= T_i \tau_x + \E[Y_i(0) \given X_i]
  \end{align*}
  $$

## Completing the Derivation

- Substitute $\E[Y_i \given T_i, X_i] = T_i \tau_x + \E[Y_i(0) \given X_i]$ into our expression:

$$
  \begin{align*}
  \widehat{\tau} &= \frac{\E\big[(T_i \tau_x + \E[Y_i(0) \given X_i])  \tilde{T}_i\big]}{\E[\tilde{T}_i^2]} \\
  &\class{fragment}{{}= \frac{\E[T_i \tau_x \tilde{T}_i] + \E[\E[Y_i(0) \given X_i] \tilde{T}_i]}{\E[\tilde{T}_i^2]}  \quad \text{($\because$ distribute)}} \\
  &\class{fragment}{{}= \frac{\E[T_i \tau_x \tilde{T}_i]}{\E[\tilde{T}_i^2]}  \quad \text{($\because$ $\E[\E[Y_i(0) \given X_i] \tilde{T}_i] = 0$ since $\E[\tilde{T}_i \given X_i] = 0$)}} \\
  &\class{fragment}{{}= \frac{\E_X[\tau_x \E[T_i \tilde{T}_i \given X_i]]}{\E_X[\E[\tilde{T}_i^2 \given X_i]]}  \quad \text{($\because$ law of iterated expectations)}} \\
  &\class{fragment}{{}= \frac{\E_X[\tau_x \E[T_i (T_i - \E[T_i \given X_i]) \given X_i] ]}{\E_X[\E[(T_i - \E[T_i \given X_i])^2 \given X_i]]}  \quad \text{($\because$ definition of $\tilde{T}_i$ )}} \\
  &\class{fragment}{{}= \frac{\E_X[\tau_x  \var(T_i \given X_i)]}{\E_X[\var(T_i \given X_i)]}  \quad \text{($\because$ $\E[T_i \tilde{T}_i \given X_i] = \var(T_i \given X_i)$)}}
  \end{align*}
$$

## What Was This?

![](../_images/phew.png){fig-align="center"}

## Comparing ATE vs. OLS Estimand

- Compare the [target]{.highlight} vs. [what OLS estimates]{.highlight}:

  $$
  \tau_{ATE} = \sum_{x} \tau_x  \Pr(X_i = x),
  $$

  versus (in binary $T_i$ case)

  $$
  \widehat{\tau} \xrightarrow{p} \frac{\E_X[\tau_x  \var(T_i \given X_i)]}{\E_X[\var(T_i \given X_i)]} = \frac{\sum_x \tau_x  \textcolor{#d65d0e}{p_x(1-p_x)}  \Pr(X_i = x)}{\sum_x \textcolor{#d65d0e}{p_x(1-p_x)}  \Pr(X_i = x)}
  $$

  where $p_x = \Pr(T_i = 1 \given X_i = x)$.

. . .

- $\widehat{\tau}$ aggregates via [conditional variance weighting]{.highlight} with respect to $T_i$ instead of just population shares.

- If $\tau_x$ was constant across $X_i$, regression recovers ATE, but variance weighting _could_ reduce efficiency (more uncertainty).

- If $T_i \indep X_i$, then $p_x(1-p_x)$ is constant across strata and cancels out, so $\widehat{\tau}$ reduces to weighting by $\Pr(X_i = x)$.

## Truth about Regression

<br>

- Logic carries through to continuous treatments [@angrist2009mostly, 77–80; @aronow2016does].

- @aronow2016does show that for arbitrary $T_i$ and $X_i$,

  $$
  \widehat{\tau} \xrightarrow{p} \frac{\E [w_i \tau_i]}{\E [w_i]}, \quad \text{where } w_i = (T_i - \E [T_i \given X_i])^2,
  $$

  in which case

  $$
  \E [w_i \given X_i] = \var [T_i \given X_i].
  $$

. . .

- The [effective sample]{.highlight} is weighted by $\widehat{w}_i = (T_i - \widehat{\E}[T_i \given X_i])^2$  (squared residual from regression of $T_i$ on covariates).

- Even with a representative sample, regression estimates may not aggregate effects in a representative manner. Regression estimates are _local_ to an effective sample.


## Let's Try a Simulation

<br>

```{r}
#| echo: true
#| eval: true
#| output: asis
#| code-line-numbers: "1-5|7-11|13-29|31-35"

set.seed(20250202) # set seed

n <- 1000 # sample size
tau_base <- 0.5
gamma <- 0.1 # effect of X on outcome

# some discrete covariate
X <- sample(x = 1:100, size = n, replace = T)

# total treatment effect (assuming possible heterogeneity)
tau_total <- sum((tau_base + 0.01 * 1:100) / 100)

# democratic institutions (correlated with confounder)
democracy_high <- rbinom(n, size = 1, prob = .5)
democracy_high_2 <-
  rbinom(n, size = 1, prob = sapply(X, function(x) .5 + 0.01 * x))

# economic growth (influenced by both investment and democratic institutions)
growth <-
  (tau_base + 0.01 * X) *
  democracy_high +
  gamma * X +
  rnorm(n, mean = 0, sd = 5)

growth_2 <-
  (tau_base + 0.01 * X) *
  democracy_high_2 +
  gamma * X +
  rnorm(n, mean = 0, sd = 5)

# regression with constant assignment
bias1 <- lm(growth ~ democracy_high + factor(X))$coefficients[2] - tau_total

# regression with variable assignment
bias2 <- lm(growth_2 ~ democracy_high_2 + factor(X))$coefficients[2] - tau_total
```

## Heterogeneous $\tau$ and Assignment

```{r}
#| label: tau_het_treat_tau
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

sims_df <-
  pbapply::pbreplicate(
    n = 1000,
    get_bias_het(),
    cl = "future"
  ) |>
  t() |>
  as.data.frame() |>
  dplyr::rename(`Variable` = het2, `Constant` = het1) |>
  pivot_longer(
    cols = c(Variable, Constant),
    names_to = "Assignment",
    values_to = "Value"
  )

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Assignment)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#cc241d", "#458588"), name = "Assignment") +
  geom_vline(
    data = summarize(group_by(sims_df, Assignment), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Assignment),
    linetype = "dashed",
    linewidth = 1
  ) +
  scale_color_manual(values = c("#cc241d", "#458588"), name = "Assignment") +
  scale_x_continuous(limits = c(-2, 2)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  )
```

## Heterogeneous Assignment Only

```{r}
#| label: tau_het_tau
#| fig-align: center
#| fig-width: 8
#| fig-height: 5

sims_df <-
  pbapply::pbreplicate(
    n = 1000,
    get_bias_het(het_fun = function(x) tau_base),
    cl = "future"
  ) |>
  t() |>
  as.data.frame() |>
  dplyr::rename(`Variable` = het2, `Constant` = het1) |>
  pivot_longer(
    cols = c(Variable, Constant),
    names_to = "Assignment",
    values_to = "Value"
  )

# Plot using ggplot2
ggplot(sims_df, aes(x = Value, fill = Assignment)) +
  geom_histogram(alpha = 0.5, position = "identity", bins = 100) +
  scale_fill_manual(values = c("#cc241d", "#458588"), name = "Assignment") +
  geom_vline(
    data = summarize(group_by(sims_df, Assignment), mean_val = mean(Value)),
    aes(xintercept = mean_val, color = Assignment),
    linetype = "dashed",
    linewidth = 1
  ) +
  scale_color_manual(values = c("#cc241d", "#458588"), name = "Assignment") +
  scale_x_continuous(limits = c(-2, 2)) +
  labs(
    x = "Deviation from True Value",
    y = "Frequency"
  )
```



## Lessons

- Regression is a useful tool for estimating causal effects and accounting for CIA:
  - **Binary treatments**: regression can provide consistent estimates of _ATE_.
  - **Discrete or continuous treatments**: Estimates provide a best linear approximation when relationship is non-linear.
  - **Heterogeneous treatments**: Regression estimates are weighted by conditional variance and could be [biased]{.highlight} and apply to [effective sample]{.highlight} only.

. . .

- Beware of [OVB]{.highlight}:
  - Avoid _"bad controls"_ that may introduce post-treatment bias or inadvertently open [back-door paths]{.highlight}.

. . .

- Steps to take:
  1. Be explicit about the assumption you need to make to make regression _causal_.
  2. Use DAGs and the back-door criterion to identify covariates to control for.
  3. Make sure you know how to interpret regression coefficients.
  4. Use simulations and/or [Dagitty](https://www.dagitty.net) to validate model assumptions and relationships.

# Appendix

## Frisch-Waugh-Lovell Theorem [🔙](#ovb) {#fwl-theorem visibility="uncounted"}

<br>

- Consider a multiple regression model: $Y_i = \alpha + \tau T_i + X_i^\prime \gamma + \nu_i$.

- To find $\widehat{\tau}$, the coefficient on $T_i$, the [Frisch-Waugh-Lovell Theorem]{.highlight} states that:

  1. Regress $Y_i$ on $X_i$ and obtain the residuals $\tilde{Y}_i = Y_i - X_i^\prime \widehat{\pi}_Y$.

  2. Regress $T_i$ on $X_i$ and obtain the residuals $\tilde{T}_i = T_i - X_i^\prime \widehat{\pi}_T$.

  3. Regress $\tilde{Y}_i$ on $\tilde{T}_i$ to obtain $\widehat{\tau}$.

  In addition, the $R^2$ and F-statistics of these regressions will be the same as those from the full model regression.

- [Intuition]{.note}:
  - The Frisch-Waugh-Lovell theorem decomposes the estimation process.
  - Adjusts $Y_i$ and $T_i$ for covariates $X_i$ separately, highlighting the direct effect of $T_i$.

## References {visibility="uncounted"}